"""
CLIP ViT Attention å¯è§†åŒ–ç¤ºä¾‹
ä½¿ç”¨æ¢¯åº¦åŠ æƒattentionæ–¹æ³•åˆ†ææ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹
"""

import os
import sys
sys.path.append('/root/autodl-tmp/benchmark_deepfakes/DeepfakeBench')

import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from transformers import CLIPImageProcessor
import yaml

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from training.utils.clip_vit_gradcam import CLIPViTGradCAM
from training.detectors.clip_enhanced import CLIPEnhanced

def load_clip_model(config_path: str, weights_path: str) -> CLIPEnhanced:
    """
    åŠ è½½CLIP Enhancedæ¨¡å‹
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        weights_path: æƒé‡æ–‡ä»¶è·¯å¾„
    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    # åŠ è½½é…ç½®
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºæ¨¡å‹
    model = CLIPEnhanced(config)
    
    # åŠ è½½æƒé‡
    if os.path.exists(weights_path):
        checkpoint = torch.load(weights_path, map_location='cpu')
        
        # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
            
        model.load_state_dict(state_dict, strict=False)
        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ: {weights_path}")
    else:
        print(f"âš ï¸  æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {weights_path}")
    
    model.eval()
    return model

def preprocess_image(image_path: str, config: dict) -> tuple:
    """
    é¢„å¤„ç†å›¾åƒ - ä½¿ç”¨å’Œè®­ç»ƒæ—¶ä¸€è‡´çš„é¢„å¤„ç†æ–¹å¼
    Args:
        image_path: å›¾åƒè·¯å¾„
        config: é…ç½®å­—å…¸
    Returns:
        (preprocessed_tensor, original_image_array)
    """
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    original_image = np.array(image)
    
    # ä½¿ç”¨å’Œè®­ç»ƒæ—¶ä¸€è‡´çš„é¢„å¤„ç†ï¼ˆæ¥è‡ªconfigå’Œpredict.pyï¼‰
    import torchvision.transforms as T
    
    transform = T.Compose([
        T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],
                            std=[0.26862954, 0.26130258, 0.27577711])
    ])
    
    # é¢„å¤„ç†ä¸ºtensor
    image_tensor = transform(image).unsqueeze(0)  # [1, 3, 224, 224]
    
    print(f"ğŸ“· å›¾åƒé¢„å¤„ç†å®Œæˆ:")
    print(f"   - åŸå§‹å°ºå¯¸: {original_image.shape}")
    print(f"   - é¢„å¤„ç†å: {image_tensor.shape}")
    print(f"   - å€¼èŒƒå›´: [{image_tensor.min():.3f}, {image_tensor.max():.3f}]")
    
    return image_tensor, original_image

def analyze_single_image(model_config_path: str, weights_path: str, image_path: str, 
                        save_dir: str = "attention_results"):
    """
    åˆ†æå•å¼ å›¾åƒçš„attentionæ¨¡å¼
    """
    print("ğŸ” å¼€å§‹å•å¼ å›¾åƒattentionåˆ†æ...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    model = load_clip_model(model_config_path, weights_path)
    
    # åˆ›å»ºGradCAM
    gradcam = CLIPViTGradCAM(
        model, 
        target_layers=[-4, -3, -2, -1],  # åˆ†ææœ€å4å±‚
        head_fusion="mean"
    )
    
    try:
        # é¢„å¤„ç†å›¾åƒ
        with open(model_config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        image_tensor, original_image = preprocess_image(image_path, config)
        
        # ç”Ÿæˆå¯è§†åŒ– - åˆ†æä¸¤ä¸ªç±»åˆ«
        image_name = os.path.splitext(os.path.basename(image_path))[0]
        
        print("ğŸ“Š åˆ†æRealç±»åˆ«çš„attention...")
        results_real = gradcam.generate_and_visualize(
            image_tensor, 
            original_image,
            target_class=0,  # Realç±»åˆ«
            save_path=os.path.join(save_dir, f"{image_name}_real_attention.png")
        )
        
        print("ğŸ“Š åˆ†æFakeç±»åˆ«çš„attention...")
        results_fake = gradcam.generate_and_visualize(
            image_tensor, 
            original_image,
            target_class=1,  # Fakeç±»åˆ«
            save_path=os.path.join(save_dir, f"{image_name}_fake_attention.png")
        )
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        create_comparison_plot(original_image, results_real, results_fake, 
                             save_path=os.path.join(save_dir, f"{image_name}_comparison.png"))
        
        print(f"âœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
        print(f"   - æ¨¡å‹é¢„æµ‹Fakeæ¦‚ç‡: {results_fake['fake_probability']:.3f}")
        print(f"   - é¢„æµ‹ç±»åˆ«: {'Fake' if results_fake['predicted_class'] else 'Real'}")
        
    finally:
        # æ¸…ç†
        gradcam.cleanup()

def create_comparison_plot(original_image: np.ndarray, results_real: dict, results_fake: dict,
                          save_path: str):
    """åˆ›å»ºReal vs Fake attentionå¯¹æ¯”å›¾"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # ç¬¬ä¸€è¡Œï¼šRealç±»åˆ«attention
    axes[0,0].imshow(original_image)
    axes[0,0].set_title('Original Image', fontsize=14)
    axes[0,0].axis('off')
    
    axes[0,1].imshow(results_real['cam'], cmap='jet')
    axes[0,1].set_title('Real Class Attention', fontsize=14)
    axes[0,1].axis('off')
    
    axes[0,2].imshow(results_real['visualization'])
    axes[0,2].set_title('Real Class Overlay', fontsize=14)
    axes[0,2].axis('off')
    
    # ç¬¬äºŒè¡Œï¼šFakeç±»åˆ«attention
    axes[1,0].imshow(original_image)
    axes[1,0].set_title('Original Image', fontsize=14)
    axes[1,0].axis('off')
    
    axes[1,1].imshow(results_fake['cam'], cmap='jet')
    axes[1,1].set_title('Fake Class Attention', fontsize=14)
    axes[1,1].axis('off')
    
    axes[1,2].imshow(results_fake['visualization'])
    axes[1,2].set_title(f'Fake Class Overlay\n(Prob: {results_fake["fake_probability"]:.3f})', fontsize=14)
    axes[1,2].axis('off')
    
    plt.suptitle('CLIP ViT Attention Analysis: Real vs Fake', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def analyze_attention_layers(model_config_path: str, weights_path: str, image_path: str,
                           save_dir: str = "layer_analysis"):
    """
    åˆ†æä¸åŒå±‚çš„attentionæ¨¡å¼
    """
    print("ğŸ” å¼€å§‹å¤šå±‚attentionåˆ†æ...")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    model = load_clip_model(model_config_path, weights_path)
    
    # é¢„å¤„ç†å›¾åƒ
    with open(model_config_path, 'r') as f:
        config = yaml.safe_load(f)
    image_tensor, original_image = preprocess_image(image_path, config)
    
    # åˆ†æä¸åŒå±‚ç»„åˆ
    layer_configs = [
        ([-1], "last_layer"),
        ([-2, -1], "last_2_layers"), 
        ([-4, -3, -2, -1], "last_4_layers"),
        ([-6, -4, -2], "spaced_layers"),
        (list(range(-12, 0)), "all_layers")  # æ‰€æœ‰å±‚
    ]
    
    fig, axes = plt.subplots(2, len(layer_configs), figsize=(4*len(layer_configs), 8))
    
    for i, (target_layers, layer_name) in enumerate(layer_configs):
        print(f"ğŸ“Š åˆ†æå±‚é…ç½®: {layer_name} (layers: {target_layers})")
        
        # åˆ›å»ºGradCAM
        gradcam = CLIPViTGradCAM(model, target_layers=target_layers, head_fusion="mean")
        
        try:
            # ç”ŸæˆCAM
            cam = gradcam.generate_cam(image_tensor, target_class=1)[0]  # Fakeç±»åˆ«
            visualization = gradcam.visualize_cam(original_image, cam)
            
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            axes[0, i].imshow(cam, cmap='jet')
            axes[0, i].set_title(f'{layer_name}\nHeatmap', fontsize=10)
            axes[0, i].axis('off')
            
            # ç»˜åˆ¶å åŠ å›¾
            axes[1, i].imshow(visualization)
            axes[1, i].set_title(f'{layer_name}\nOverlay', fontsize=10)
            axes[1, i].axis('off')
            
        finally:
            gradcam.cleanup()
    
    plt.suptitle('Multi-Layer Attention Analysis', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'layer_comparison.png'), dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ä¸»å‡½æ•° - ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„æ¥è¿è¡Œä½ çš„åˆ†æ"""
    
    # ğŸ”§ é…ç½®è·¯å¾„ - è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹
    model_config_path = "training/config/detector/clip_enhanced.yaml"
    #weights_path = "training/FaceForensics++/ckpt_epoch_9_best.pth"  # ä½ çš„æ¨¡å‹æƒé‡è·¯å¾„
    weights_path = "./logs/training/clip_enhanced_2025-06-01-19-22-52/test/avg/ckpt_best.pth"
    
    # æµ‹è¯•å›¾åƒè·¯å¾„ - è¯·ä½¿ç”¨ä½ è¦åˆ†æçš„å›¾åƒ
    #test_image_path = "test_image.jpg"  # è¯·æ›¿æ¢ä¸ºå®é™…å›¾åƒè·¯å¾„
    test_image_path = "/root/autodl-tmp/benchmark_deepfakes/DeepfakeBench/datasets/rgb/FaceForensics++/manipulated_sequences/FaceSwap/c23/frames/000_003/009.png"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {model_config_path}")
        return
    
    if not os.path.exists(test_image_path):
        print(f"âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image_path}")
        print("è¯·å°†ä½ è¦åˆ†æçš„å›¾åƒæ”¾åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œå¹¶å‘½åä¸ºtest_image.jpg")
        return
    
    try:
        print("ğŸš€ å¼€å§‹CLIP ViT Attentionå¯è§†åŒ–åˆ†æ...")
        
        # 1. å•å¼ å›¾åƒåˆ†æ
        analyze_single_image(model_config_path, weights_path, test_image_path)
        
        # 2. å¤šå±‚åˆ†æ (å¯é€‰)
        print("\n" + "="*60)
        response = input("æ˜¯å¦è¿›è¡Œå¤šå±‚attentionåˆ†æï¼Ÿ(y/n): ")
        if response.lower() == 'y':
            analyze_attention_layers(model_config_path, weights_path, test_image_path)
        
        print("\nâœ… æ‰€æœ‰åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
