+ python3 training/test.py --detector_path ./training/config/detector/clip_stan.yaml --test_dataset DFDC DFDCP Celeb-DF-v2 UADFV --weights_path ./logs/training/clip_stan_2025-05-30-23-08-08/test/avg/ckpt_best.pth
/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'groups', 'bias', 'padding_mode', 'device', 'dtype']
spatial_count=0 keep_stride_count=0
batch_size: 32
Skipping video DFDC_Real_hmvjiodnxy because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_zffenlgire because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_mpwoskwuuk because it has less than clip_size (8) frames (7).
Skipping video DFDC_Real_hqfpcnnxuj because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_iyfwndboqs because it has less than clip_size (8) frames (3).
Skipping video DFDC_Real_issioyofmn because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_xgtelcltvg because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_vytzcppkpg because it has less than clip_size (8) frames (2).
Skipping video DFDC_Real_gqqhtdjoha because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_nodoppwmyj because it has less than clip_size (8) frames (3).
Skipping video DFDC_Real_bsduhevcwg because it has less than clip_size (8) frames (7).
Skipping video DFDC_Real_pgbtmrrmrq because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_dxuxornpue because it has less than clip_size (8) frames (6).
Skipping video DFDC_Real_rpqkwlaasy because it has less than clip_size (8) frames (7).
Skipping video DFDC_Real_usrimdndpz because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_rodpvqvdxm because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_fagregozex because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_aalscayrfi because it has less than clip_size (8) frames (3).
Skipping video DFDC_Real_sygzijgurs because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_qvpjagljpr because it has less than clip_size (8) frames (3).
Skipping video DFDC_Real_puhcxsfnde because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_rafzcxkbsh because it has less than clip_size (8) frames (7).
Skipping video DFDC_Real_olanckxfdx because it has less than clip_size (8) frames (6).
Skipping video DFDC_Real_cemvsipmew because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_egxsckqubp because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_hmmnljhtom because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_gdnacoubws because it has less than clip_size (8) frames (2).
Skipping video DFDC_Real_xvkflltlbg because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_ejunkkahbw because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_snsnifxref because it has less than clip_size (8) frames (3).
Skipping video DFDC_Real_zztotvpkjc because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_eppwzalgil because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_xnwmohspsh because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_pwfhdwzxqm because it has less than clip_size (8) frames (2).
Skipping video DFDC_Real_igbpkykmrz because it has less than clip_size (8) frames (2).
Skipping video DFDC_Real_apmxeenhpt because it has less than clip_size (8) frames (6).
Skipping video DFDC_Real_sfuddhcgmz because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_kdiezumwzi because it has less than clip_size (8) frames (2).
Skipping video DFDC_Real_vxvqnhakrk because it has less than clip_size (8) frames (7).
Skipping video DFDC_Real_bzqohcbrip because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_ovyikmbbdr because it has less than clip_size (8) frames (5).
Skipping video DFDC_Real_nuhvoaxqtn because it has less than clip_size (8) frames (2).
Skipping video DFDC_Real_ovbvtbmuxo because it has less than clip_size (8) frames (1).
Skipping video DFDC_Real_knamsitmul because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_mipljmaefr because it has less than clip_size (8) frames (4).
Skipping video DFDC_Real_hswqufrqdq because it has less than clip_size (8) frames (6).
Skipping video DFDC_Fake_ljaedxbwki because it has less than clip_size (8) frames (6).
Skipping video DFDC_Fake_gkhhhvjkmr because it has less than clip_size (8) frames (5).
Skipping video DFDC_Fake_xirozlwlpd because it has less than clip_size (8) frames (1).
Skipping video DFDC_Fake_noodanjabq because it has less than clip_size (8) frames (1).
Skipping video DFDC_Fake_ydiydkhofb because it has less than clip_size (8) frames (1).
Skipping video DFDC_Fake_nrzayygpza because it has less than clip_size (8) frames (7).
Skipping video DFDC_Fake_wpotxtrxtj because it has less than clip_size (8) frames (2).
Skipping video DFDC_Fake_eboyboyzis because it has less than clip_size (8) frames (4).
Skipping video DFDC_Fake_jqfpwtawdw because it has less than clip_size (8) frames (2).
Skipping video DFDC_Fake_zfbfgatuvk because it has less than clip_size (8) frames (1).
Skipping video DFDC_Fake_ejhumipcge because it has less than clip_size (8) frames (2).
Skipping video DFDC_Fake_pcnvtisyms because it has less than clip_size (8) frames (6).
Skipping video DFDC_Fake_hmwdlvrwbe because it has less than clip_size (8) frames (4).
Skipping video DFDC_Fake_bxgfrpxpyp because it has less than clip_size (8) frames (6).
Skipping video DFDC_Fake_wxorluvmvm because it has less than clip_size (8) frames (2).
Skipping video DFDC_Fake_jgmmablvoo because it has less than clip_size (8) frames (6).
Skipping video DFDC_Fake_gneonsmngz because it has less than clip_size (8) frames (2).
Skipping video DFDC_Fake_okogoesrwg because it has less than clip_size (8) frames (6).
Skipping video DFDC_Fake_wuzzdfwxlv because it has less than clip_size (8) frames (5).
Skipping video DFDC_Fake_jarlmlkfam because it has less than clip_size (8) frames (3).
Skipping video DFDC_Fake_shjmzxlkgk because it has less than clip_size (8) frames (1).
Skipping video DFDC_Fake_hrijskleak because it has less than clip_size (8) frames (4).
Skipping video DFDC_Fake_mzldnycwvi because it has less than clip_size (8) frames (1).
Skipping video DFDC_Fake_szkejtkgqq because it has less than clip_size (8) frames (7).
Skipping video DFDC_Fake_gowytswfqh because it has less than clip_size (8) frames (6).
Skipping video DFDC_Fake_ikgvwddrng because it has less than clip_size (8) frames (7).
Skipping video DFDCP_Real_1152039_B_002 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_Real_1152039_E_001 because it has less than clip_size (8) frames (5).
Skipping video DFDCP_Real_1290777_C_002 because it has less than clip_size (8) frames (5).
Skipping video DFDCP_Real_1433884_C_001 because it has less than clip_size (8) frames (6).
Skipping video DFDCP_Real_1848521_E_001 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_Real_1851350_C_001 because it has less than clip_size (8) frames (7).
Skipping video DFDCP_Real_1851350_C_002 because it has less than clip_size (8) frames (3).
Skipping video DFDCP_Real_1869846_D_001 because it has less than clip_size (8) frames (6).
Skipping video DFDCP_Real_1869846_D_002 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_Real_1869846_D_003 because it has less than clip_size (8) frames (3).
Skipping video DFDCP_Real_1873849_B_001 because it has less than clip_size (8) frames (6).
Skipping video DFDCP_Real_1873849_B_002 because it has less than clip_size (8) frames (7).
Skipping video DFDCP_Real_1873849_B_003 because it has less than clip_size (8) frames (6).
Skipping video DFDCP_Real_2005778_F_003 because it has less than clip_size (8) frames (5).
Skipping video DFDCP_Real_2005778_I_002 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_Real_2005778_K_003 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_Real_2022094_A_001 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_Real_2022094_D_002 because it has less than clip_size (8) frames (4).
Skipping video DFDCP_Real_2031720_A_003 because it has less than clip_size (8) frames (4).
Skipping video DFDCP_Real_2090100_C_001 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_Real_643049_C_002 because it has less than clip_size (8) frames (5).
Skipping video DFDCP_Real_643049_C_003 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_Real_643049_D_002 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_Real_643049_D_003 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_Real_643049_E_003 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_1851350_1390015_A_003 because it has less than clip_size (8) frames (7).
Skipping video DFDCP_FakeA_2040724_1441897_A_003 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_1848521_1600085_A_003 because it has less than clip_size (8) frames (7).
Skipping video DFDCP_FakeA_2031720_1600085_A_003 because it has less than clip_size (8) frames (6).
Skipping video DFDCP_FakeA_1441897_1848521_B_000 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_FakeA_2040724_1848521_B_000 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_FakeA_2040724_1848521_B_002 because it has less than clip_size (8) frames (4).
Skipping video DFDCP_FakeA_2090100_2005778_C_002 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_1354471_2022094_A_003 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_2084869_2022094_A_000 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_2084869_2022094_A_003 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_2090100_2022094_A_000 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_1782722_2022094_C_002 because it has less than clip_size (8) frames (5).
Skipping video DFDCP_FakeA_1828891_2022094_D_003 because it has less than clip_size (8) frames (7).
Skipping video DFDCP_FakeA_2090100_2022094_D_000 because it has less than clip_size (8) frames (6).
Skipping video DFDCP_FakeA_2090100_2022094_D_002 because it has less than clip_size (8) frames (7).
Skipping video DFDCP_FakeA_1782722_2090100_A_003 because it has less than clip_size (8) frames (6).
Skipping video DFDCP_FakeA_1782722_643049_C_000 because it has less than clip_size (8) frames (3).
Skipping video DFDCP_FakeA_2005778_643049_C_000 because it has less than clip_size (8) frames (5).
Skipping video DFDCP_FakeA_2084869_643049_C_003 because it has less than clip_size (8) frames (2).
Skipping video DFDCP_FakeA_2090100_643049_C_000 because it has less than clip_size (8) frames (1).
Skipping video DFDCP_FakeA_2090100_643049_C_003 because it has less than clip_size (8) frames (1).
Skipping video CelebDFv2_real_id10_0001 because it has less than clip_size (8) frames (3).
clip_path: weights/clip-vit-base-patch16
features_dim: 768

All parameters:
class_embedding shape = (768,)
cls_token shape = (1, 1, 768)
feature_extractor.vision_model.embeddings.patch_embedding.weight shape = (768, 3, 16, 16)
feature_extractor.vision_model.embeddings.position_embedding.weight shape = (197, 768)
feature_extractor.vision_model.pre_layrnorm.weight shape = (768,)
feature_extractor.vision_model.pre_layrnorm.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.0.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.0.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.0.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.1.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.1.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.1.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.2.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.2.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.2.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.3.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.3.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.3.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.4.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.4.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.4.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.5.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.5.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.5.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.6.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.6.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.6.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.7.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.7.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.7.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.8.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.8.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.8.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.9.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.9.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.9.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.10.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.10.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.10.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.11.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.11.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.11.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.post_layernorm.weight shape = (768,)
feature_extractor.vision_model.post_layernorm.bias shape = (768,)
STAN_S_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.0.layer_norm1.weight shape = (768,)
STAN_S_layers.0.layer_norm1.bias shape = (768,)
STAN_S_layers.0.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.0.mlp.fc1.bias shape = (3072,)
STAN_S_layers.0.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.0.mlp.fc2.bias shape = (768,)
STAN_S_layers.0.layer_norm2.weight shape = (768,)
STAN_S_layers.0.layer_norm2.bias shape = (768,)
STAN_S_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.1.layer_norm1.weight shape = (768,)
STAN_S_layers.1.layer_norm1.bias shape = (768,)
STAN_S_layers.1.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.1.mlp.fc1.bias shape = (3072,)
STAN_S_layers.1.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.1.mlp.fc2.bias shape = (768,)
STAN_S_layers.1.layer_norm2.weight shape = (768,)
STAN_S_layers.1.layer_norm2.bias shape = (768,)
STAN_S_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.2.layer_norm1.weight shape = (768,)
STAN_S_layers.2.layer_norm1.bias shape = (768,)
STAN_S_layers.2.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.2.mlp.fc1.bias shape = (3072,)
STAN_S_layers.2.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.2.mlp.fc2.bias shape = (768,)
STAN_S_layers.2.layer_norm2.weight shape = (768,)
STAN_S_layers.2.layer_norm2.bias shape = (768,)
STAN_S_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.3.layer_norm1.weight shape = (768,)
STAN_S_layers.3.layer_norm1.bias shape = (768,)
STAN_S_layers.3.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.3.mlp.fc1.bias shape = (3072,)
STAN_S_layers.3.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.3.mlp.fc2.bias shape = (768,)
STAN_S_layers.3.layer_norm2.weight shape = (768,)
STAN_S_layers.3.layer_norm2.bias shape = (768,)
STAN_T_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.0.layer_norm1.weight shape = (768,)
STAN_T_layers.0.layer_norm1.bias shape = (768,)
STAN_T_layers.0.temporal_fc.weight shape = (768, 768)
STAN_T_layers.0.temporal_fc.bias shape = (768,)
STAN_T_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.1.layer_norm1.weight shape = (768,)
STAN_T_layers.1.layer_norm1.bias shape = (768,)
STAN_T_layers.1.temporal_fc.weight shape = (768, 768)
STAN_T_layers.1.temporal_fc.bias shape = (768,)
STAN_T_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.2.layer_norm1.weight shape = (768,)
STAN_T_layers.2.layer_norm1.bias shape = (768,)
STAN_T_layers.2.temporal_fc.weight shape = (768, 768)
STAN_T_layers.2.temporal_fc.bias shape = (768,)
STAN_T_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.3.layer_norm1.weight shape = (768,)
STAN_T_layers.3.layer_norm1.bias shape = (768,)
STAN_T_layers.3.temporal_fc.weight shape = (768, 768)
STAN_T_layers.3.temporal_fc.bias shape = (768,)
time_embed.weight shape = (64, 768)
model.linear.weight shape = (2, 768)
model.linear.bias shape = (2,)

ğŸ”¥ Trainable parameters:
class_embedding shape = (768,)
cls_token shape = (1, 1, 768)
feature_extractor.vision_model.pre_layrnorm.weight shape = (768,)
feature_extractor.vision_model.pre_layrnorm.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.post_layernorm.weight shape = (768,)
feature_extractor.vision_model.post_layernorm.bias shape = (768,)
STAN_S_layers.0.layer_norm1.weight shape = (768,)
STAN_S_layers.0.layer_norm1.bias shape = (768,)
STAN_S_layers.0.layer_norm2.weight shape = (768,)
STAN_S_layers.0.layer_norm2.bias shape = (768,)
STAN_S_layers.1.layer_norm1.weight shape = (768,)
STAN_S_layers.1.layer_norm1.bias shape = (768,)
STAN_S_layers.1.layer_norm2.weight shape = (768,)
STAN_S_layers.1.layer_norm2.bias shape = (768,)
STAN_S_layers.2.layer_norm1.weight shape = (768,)
STAN_S_layers.2.layer_norm1.bias shape = (768,)
STAN_S_layers.2.layer_norm2.weight shape = (768,)
STAN_S_layers.2.layer_norm2.bias shape = (768,)
STAN_S_layers.3.layer_norm1.weight shape = (768,)
STAN_S_layers.3.layer_norm1.bias shape = (768,)
STAN_S_layers.3.layer_norm2.weight shape = (768,)
STAN_S_layers.3.layer_norm2.bias shape = (768,)
STAN_T_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.0.layer_norm1.weight shape = (768,)
STAN_T_layers.0.layer_norm1.bias shape = (768,)
STAN_T_layers.0.temporal_fc.weight shape = (768, 768)
STAN_T_layers.0.temporal_fc.bias shape = (768,)
STAN_T_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.1.layer_norm1.weight shape = (768,)
STAN_T_layers.1.layer_norm1.bias shape = (768,)
STAN_T_layers.1.temporal_fc.weight shape = (768, 768)
STAN_T_layers.1.temporal_fc.bias shape = (768,)
STAN_T_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.2.layer_norm1.weight shape = (768,)
STAN_T_layers.2.layer_norm1.bias shape = (768,)
STAN_T_layers.2.temporal_fc.weight shape = (768, 768)
STAN_T_layers.2.temporal_fc.bias shape = (768,)
STAN_T_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.3.layer_norm1.weight shape = (768,)
STAN_T_layers.3.layer_norm1.bias shape = (768,)
STAN_T_layers.3.temporal_fc.weight shape = (768, 768)
STAN_T_layers.3.temporal_fc.bias shape = (768,)
time_embed.weight shape = (64, 768)
model.linear.weight shape = (2, 768)
model.linear.bias shape = (2,)
Total parameters: 126020354, trainable: 11922434, %: 9.4607
===> Load checkpoint done!

Dataset: DFDC
Number of images: 3151
Number of labels: 3151
test images: 3151

  0%|          | 0/99 [00:00<?, ?it/s]
  1%|          | 1/99 [00:06<10:23,  6.36s/it]
  2%|â–         | 2/99 [00:07<05:35,  3.46s/it]
  3%|â–         | 3/99 [00:09<04:03,  2.54s/it]
  4%|â–         | 4/99 [00:10<03:19,  2.10s/it]
  5%|â–Œ         | 5/99 [00:12<02:54,  1.86s/it]
  6%|â–Œ         | 6/99 [00:13<02:39,  1.71s/it]
  7%|â–‹         | 7/99 [00:14<02:28,  1.62s/it]
  8%|â–Š         | 8/99 [00:16<02:21,  1.56s/it]
  9%|â–‰         | 9/99 [00:17<02:16,  1.52s/it]
 10%|â–ˆ         | 10/99 [00:19<02:12,  1.49s/it]
 11%|â–ˆ         | 11/99 [00:20<02:09,  1.47s/it]
 12%|â–ˆâ–        | 12/99 [00:22<02:07,  1.46s/it]
 13%|â–ˆâ–        | 13/99 [00:23<02:04,  1.45s/it]
 14%|â–ˆâ–        | 14/99 [00:24<02:02,  1.45s/it]
 15%|â–ˆâ–Œ        | 15/99 [00:26<02:01,  1.44s/it]
 16%|â–ˆâ–Œ        | 16/99 [00:27<01:59,  1.44s/it]
 17%|â–ˆâ–‹        | 17/99 [00:29<01:58,  1.44s/it]
 18%|â–ˆâ–Š        | 18/99 [00:30<01:56,  1.44s/it]
 19%|â–ˆâ–‰        | 19/99 [00:32<01:55,  1.44s/it]
 20%|â–ˆâ–ˆ        | 20/99 [00:33<01:53,  1.44s/it]
 21%|â–ˆâ–ˆ        | 21/99 [00:35<01:51,  1.43s/it]
 22%|â–ˆâ–ˆâ–       | 22/99 [00:36<01:50,  1.43s/it]
 23%|â–ˆâ–ˆâ–       | 23/99 [00:37<01:49,  1.44s/it]
 24%|â–ˆâ–ˆâ–       | 24/99 [00:39<01:47,  1.44s/it]
 25%|â–ˆâ–ˆâ–Œ       | 25/99 [00:40<01:46,  1.44s/it]
 26%|â–ˆâ–ˆâ–‹       | 26/99 [00:42<01:45,  1.44s/it]
 27%|â–ˆâ–ˆâ–‹       | 27/99 [00:43<01:44,  1.45s/it]
 28%|â–ˆâ–ˆâ–Š       | 28/99 [00:45<01:43,  1.45s/it]
 29%|â–ˆâ–ˆâ–‰       | 29/99 [00:46<01:41,  1.45s/it]
 30%|â–ˆâ–ˆâ–ˆ       | 30/99 [00:48<01:40,  1.45s/it]
 31%|â–ˆâ–ˆâ–ˆâ–      | 31/99 [00:49<01:38,  1.45s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 32/99 [00:50<01:37,  1.45s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 33/99 [00:52<01:35,  1.45s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 34/99 [00:53<01:34,  1.45s/it]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/99 [00:55<01:32,  1.45s/it]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 36/99 [00:56<01:31,  1.45s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/99 [00:58<01:29,  1.45s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/99 [00:59<01:28,  1.45s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/99 [01:01<01:26,  1.45s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/99 [01:02<01:25,  1.45s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 41/99 [01:04<01:24,  1.45s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/99 [01:05<01:22,  1.45s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/99 [01:06<01:21,  1.45s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/99 [01:08<01:19,  1.45s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/99 [01:09<01:18,  1.45s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 46/99 [01:11<01:16,  1.45s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/99 [01:12<01:15,  1.45s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/99 [01:14<01:13,  1.45s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/99 [01:15<01:12,  1.45s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/99 [01:17<01:11,  1.45s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 51/99 [01:18<01:09,  1.46s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/99 [01:19<01:08,  1.46s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/99 [01:21<01:07,  1.46s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/99 [01:22<01:05,  1.46s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/99 [01:24<01:04,  1.46s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 56/99 [01:25<01:02,  1.46s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 57/99 [01:27<01:01,  1.46s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/99 [01:28<00:59,  1.46s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/99 [01:30<00:58,  1.46s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/99 [01:31<00:56,  1.46s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/99 [01:33<00:55,  1.46s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/99 [01:34<00:53,  1.46s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/99 [01:36<00:52,  1.46s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/99 [01:37<00:51,  1.46s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/99 [01:38<00:49,  1.46s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 66/99 [01:40<00:48,  1.46s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 67/99 [01:41<00:46,  1.46s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/99 [01:43<00:45,  1.46s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/99 [01:44<00:43,  1.46s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/99 [01:46<00:42,  1.45s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/99 [01:47<00:40,  1.45s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/99 [01:49<00:39,  1.46s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/99 [01:50<00:37,  1.46s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/99 [01:52<00:36,  1.45s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/99 [01:53<00:34,  1.46s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 76/99 [01:55<00:33,  1.46s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 77/99 [01:56<00:32,  1.46s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 78/99 [01:57<00:30,  1.46s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/99 [01:59<00:29,  1.46s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/99 [02:00<00:27,  1.46s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81/99 [02:02<00:26,  1.46s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/99 [02:03<00:24,  1.46s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/99 [02:05<00:23,  1.46s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/99 [02:06<00:21,  1.46s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/99 [02:08<00:20,  1.46s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 86/99 [02:09<00:18,  1.46s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 87/99 [02:11<00:17,  1.46s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 88/99 [02:12<00:16,  1.46s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/99 [02:13<00:14,  1.46s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/99 [02:15<00:13,  1.46s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91/99 [02:16<00:11,  1.46s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/99 [02:18<00:10,  1.46s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93/99 [02:19<00:08,  1.46s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/99 [02:21<00:07,  1.46s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/99 [02:22<00:05,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 96/99 [02:24<00:04,  1.46s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 97/99 [02:25<00:02,  1.46s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 98/99 [02:27<00:01,  1.46s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [02:28<00:00,  1.41s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [02:28<00:00,  1.50s/it]

Detailed metrics for DFDC:
True Negatives (Real classified as Real): 779
False Positives (Real classified as Fake): 703
False Negatives (Fake classified as Real): 127
True Positives (Fake classified as Fake): 1542
Accuracy: 0.7366
Precision: 0.6869
Recall: 0.9239
F1 Score: 0.7879

Predictions shape: (3151,)
Labels shape: (3151,)
Number of processed images: 3151
Number of processed image names: 3151
len(y_pred): 3151
len(y_true): 3151

dataset: DFDC
acc: 0.7365915582354808
auc: 0.8537395823984073
eer: 0.23751686909581646
ap: 0.8780520711349282
pred: [0.90797555 0.79342484 0.46184188 ... 0.6102086  0.829188   0.68525326]
video_auc: 0.8537395823984073
label: [1 1 0 ... 1 1 0]

Dataset: DFDCP
Number of images: 1996
Number of labels: 1996
test images: 1996

  0%|          | 0/63 [00:00<?, ?it/s]
  2%|â–         | 1/63 [00:04<04:42,  4.56s/it]
  3%|â–         | 2/63 [00:06<02:52,  2.82s/it]
  5%|â–         | 3/63 [00:07<02:12,  2.20s/it]
  6%|â–‹         | 4/63 [00:09<01:52,  1.91s/it]
  8%|â–Š         | 5/63 [00:10<01:41,  1.74s/it]
 10%|â–‰         | 6/63 [00:11<01:33,  1.64s/it]
 11%|â–ˆ         | 7/63 [00:13<01:28,  1.58s/it]
 13%|â–ˆâ–        | 8/63 [00:14<01:24,  1.54s/it]
 14%|â–ˆâ–        | 9/63 [00:16<01:21,  1.51s/it]
 16%|â–ˆâ–Œ        | 10/63 [00:17<01:18,  1.49s/it]
 17%|â–ˆâ–‹        | 11/63 [00:19<01:16,  1.48s/it]
 19%|â–ˆâ–‰        | 12/63 [00:20<01:15,  1.47s/it]
 21%|â–ˆâ–ˆ        | 13/63 [00:22<01:13,  1.47s/it]
 22%|â–ˆâ–ˆâ–       | 14/63 [00:23<01:11,  1.46s/it]
 24%|â–ˆâ–ˆâ–       | 15/63 [00:25<01:09,  1.46s/it]
 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:26<01:08,  1.46s/it]
 27%|â–ˆâ–ˆâ–‹       | 17/63 [00:27<01:06,  1.45s/it]
 29%|â–ˆâ–ˆâ–Š       | 18/63 [00:29<01:05,  1.45s/it]
 30%|â–ˆâ–ˆâ–ˆ       | 19/63 [00:30<01:04,  1.46s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [00:32<01:02,  1.46s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 21/63 [00:33<01:01,  1.46s/it]
 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [00:35<00:59,  1.46s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/63 [00:36<00:58,  1.45s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [00:38<00:56,  1.45s/it]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 25/63 [00:39<00:55,  1.46s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [00:41<00:54,  1.46s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/63 [00:42<00:52,  1.46s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [00:43<00:51,  1.46s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/63 [00:45<00:49,  1.46s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [00:46<00:47,  1.45s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 31/63 [00:48<00:46,  1.45s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [00:49<00:45,  1.45s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/63 [00:51<00:43,  1.45s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [00:52<00:42,  1.45s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 35/63 [00:54<00:40,  1.46s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [00:55<00:39,  1.46s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/63 [00:57<00:37,  1.46s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [00:58<00:36,  1.46s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 39/63 [00:59<00:34,  1.46s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/63 [01:01<00:33,  1.46s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 41/63 [01:02<00:32,  1.45s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [01:04<00:30,  1.46s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 43/63 [01:05<00:29,  1.46s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [01:07<00:27,  1.45s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 45/63 [01:08<00:26,  1.45s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/63 [01:10<00:24,  1.45s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/63 [01:11<00:23,  1.45s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [01:13<00:21,  1.45s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 49/63 [01:14<00:20,  1.45s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [01:15<00:18,  1.46s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 51/63 [01:17<00:17,  1.46s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/63 [01:18<00:16,  1.46s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/63 [01:20<00:14,  1.46s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [01:21<00:13,  1.46s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 55/63 [01:23<00:11,  1.46s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [01:24<00:10,  1.46s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 57/63 [01:26<00:08,  1.46s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [01:27<00:07,  1.46s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/63 [01:29<00:05,  1.46s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [01:30<00:04,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 61/63 [01:32<00:02,  1.46s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [01:33<00:01,  1.46s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:34<00:00,  1.34s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:34<00:00,  1.51s/it]

Detailed metrics for DFDCP:
True Negatives (Real classified as Real): 295
False Positives (Real classified as Fake): 381
False Negatives (Fake classified as Real): 119
True Positives (Fake classified as Fake): 1201
Accuracy: 0.7495
Precision: 0.7592
Recall: 0.9098
F1 Score: 0.8277

Predictions shape: (1996,)
Labels shape: (1996,)
Number of processed images: 1996
Number of processed image names: 1996
len(y_pred): 1996
len(y_true): 1996

dataset: DFDCP
acc: 0.749498997995992
auc: 0.8282925407925407
eer: 0.26479289940828404
ap: 0.9043693825603686
pred: [0.7815539  0.6475797  0.9304718  ... 0.5749017  0.47490603 0.5040473 ]
video_auc: 0.8282925407925407
label: [1 1 1 ... 0 1 0]

Dataset: Celeb-DF-v2
Number of images: 2022
Number of labels: 2022
test images: 2022

  0%|          | 0/64 [00:00<?, ?it/s]
  2%|â–         | 1/64 [00:04<04:33,  4.34s/it]
  3%|â–         | 2/64 [00:05<02:49,  2.73s/it]
  5%|â–         | 3/64 [00:07<02:11,  2.16s/it]
  6%|â–‹         | 4/64 [00:08<01:53,  1.88s/it]
  8%|â–Š         | 5/64 [00:10<01:42,  1.74s/it]
  9%|â–‰         | 6/64 [00:11<01:35,  1.64s/it]
 11%|â–ˆ         | 7/64 [00:13<01:30,  1.58s/it]
 12%|â–ˆâ–        | 8/64 [00:14<01:26,  1.54s/it]
 14%|â–ˆâ–        | 9/64 [00:16<01:23,  1.52s/it]
 16%|â–ˆâ–Œ        | 10/64 [00:17<01:21,  1.50s/it]
 17%|â–ˆâ–‹        | 11/64 [00:19<01:18,  1.49s/it]
 19%|â–ˆâ–‰        | 12/64 [00:20<01:16,  1.48s/it]
 20%|â–ˆâ–ˆ        | 13/64 [00:22<01:15,  1.47s/it]
 22%|â–ˆâ–ˆâ–       | 14/64 [00:23<01:13,  1.47s/it]
 23%|â–ˆâ–ˆâ–       | 15/64 [00:24<01:11,  1.47s/it]
 25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:26<01:10,  1.46s/it]
 27%|â–ˆâ–ˆâ–‹       | 17/64 [00:27<01:08,  1.46s/it]
 28%|â–ˆâ–ˆâ–Š       | 18/64 [00:29<01:07,  1.46s/it]
 30%|â–ˆâ–ˆâ–‰       | 19/64 [00:30<01:05,  1.46s/it]
 31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:32<01:04,  1.46s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 21/64 [00:33<01:02,  1.46s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:35<01:01,  1.46s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:36<00:59,  1.46s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:38<00:58,  1.46s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:39<00:57,  1.46s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:41<00:55,  1.47s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:42<00:54,  1.46s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:43<00:52,  1.46s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:45<00:51,  1.46s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [00:46<00:49,  1.46s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:48<00:48,  1.46s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/64 [00:49<00:46,  1.46s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/64 [00:51<00:45,  1.46s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/64 [00:52<00:43,  1.46s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:54<00:42,  1.46s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/64 [00:55<00:40,  1.46s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/64 [00:57<00:39,  1.46s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 38/64 [00:58<00:37,  1.46s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 39/64 [01:00<00:36,  1.46s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/64 [01:01<00:35,  1.46s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/64 [01:02<00:33,  1.46s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 42/64 [01:04<00:32,  1.46s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 43/64 [01:05<00:30,  1.46s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [01:07<00:29,  1.46s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 45/64 [01:08<00:27,  1.46s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/64 [01:10<00:26,  1.46s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/64 [01:11<00:24,  1.46s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [01:13<00:23,  1.46s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [01:14<00:21,  1.46s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 50/64 [01:16<00:20,  1.46s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 51/64 [01:17<00:18,  1.46s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [01:18<00:17,  1.46s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/64 [01:20<00:16,  1.46s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 54/64 [01:21<00:14,  1.46s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 55/64 [01:23<00:13,  1.46s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 56/64 [01:24<00:11,  1.46s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [01:26<00:10,  1.46s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 58/64 [01:27<00:08,  1.46s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [01:29<00:07,  1.46s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 60/64 [01:30<00:05,  1.46s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [01:32<00:04,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 62/64 [01:33<00:02,  1.47s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 63/64 [01:35<00:01,  1.46s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [01:35<00:00,  1.20s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [01:35<00:00,  1.50s/it]

Detailed metrics for Celeb-DF-v2:
True Negatives (Real classified as Real): 206
False Positives (Real classified as Fake): 477
False Negatives (Fake classified as Real): 74
True Positives (Fake classified as Fake): 1265
Accuracy: 0.7275
Precision: 0.7262
Recall: 0.9447
F1 Score: 0.8212

Predictions shape: (2022,)
Labels shape: (2022,)
Number of processed images: 2022
Number of processed image names: 2022
len(y_pred): 2022
len(y_true): 2022

dataset: Celeb-DF-v2
acc: 0.7274975272007913
auc: 0.8135854536229808
eer: 0.2562225475841874
ap: 0.894890005367688
pred: [0.5142923 0.4508252 0.8720841 ... 0.8600651 0.9278985 0.8817606]
video_auc: 0.8135854536229808
label: [1 1 1 ... 1 1 1]

Dataset: UADFV
Number of images: 382
Number of labels: 382
test images: 382

  0%|          | 0/12 [00:00<?, ?it/s]
  8%|â–Š         | 1/12 [00:02<00:30,  2.80s/it]
 17%|â–ˆâ–‹        | 2/12 [00:04<00:20,  2.01s/it]
 25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:05<00:15,  1.76s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 4/12 [00:07<00:13,  1.64s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:08<00:11,  1.57s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:10<00:09,  1.53s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:11<00:07,  1.50s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:12<00:05,  1.49s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:14<00:04,  1.48s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 10/12 [00:15<00:02,  1.47s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:17<00:01,  1.47s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:19<00:00,  1.79s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:20<00:00,  1.67s/it]

Detailed metrics for UADFV:
True Negatives (Real classified as Real): 50
False Positives (Real classified as Fake): 140
False Negatives (Fake classified as Real): 0
True Positives (Fake classified as Fake): 192
Accuracy: 0.6335
Precision: 0.5783
Recall: 1.0000
F1 Score: 0.7328

Predictions shape: (382,)
Labels shape: (382,)
Number of processed images: 382
Number of processed image names: 382
len(y_pred): 382
len(y_true): 382

dataset: UADFV
acc: 0.6335078534031413
auc: 0.9564418859649123
eer: 0.10526315789473684
ap: 0.9536349362959898
pred: [0.5705279  0.9242852  0.7446398  0.9310035  0.5227114  0.9343785
 0.6197153  0.72989327 0.9218679  0.48847303 0.91798925 0.9333314
 0.8934803  0.8879613  0.93352085 0.881103   0.90003306 0.8257045
 0.89754874 0.930437   0.89294475 0.48761362 0.9329904  0.64073414
 0.7244304  0.91155624 0.87714285 0.6744379  0.88880867 0.57841283
 0.7764715  0.76969033 0.5020694  0.8395016  0.4398914  0.93436587
 0.911226   0.93108165 0.5487417  0.9302485  0.86171955 0.5431064
 0.51446337 0.9341208  0.5750461  0.816374   0.92566496 0.86778
 0.8408475  0.6618946  0.8974987  0.5723037  0.74468124 0.8023507
 0.72099286 0.57200986 0.42956278 0.4126301  0.887085   0.9049171
 0.91337216 0.9202997  0.75893843 0.70999223 0.8900906  0.86496276
 0.48934066 0.9196561  0.9337456  0.825179   0.9204884  0.92589605
 0.87819463 0.92364985 0.54969275 0.92308533 0.81172484 0.48099244
 0.8496825  0.929771   0.4777003  0.9185194  0.8965275  0.88512236
 0.5896923  0.6153016  0.91403985 0.9307833  0.90077156 0.89120096
 0.67230445 0.73630464 0.9362022  0.4122918  0.71874607 0.87078446
 0.4680615  0.89800483 0.7667773  0.93654907 0.86741316 0.92075574
 0.45733812 0.41605112 0.6882508  0.8773371  0.6432155  0.927114
 0.9105643  0.7059509  0.84207374 0.46193516 0.88635665 0.5161513
 0.46378255 0.61233306 0.87949634 0.49094048 0.4164266  0.59007543
 0.7535969  0.6629572  0.41737536 0.8461475  0.9334441  0.9190012
 0.4511899  0.7056039  0.92077297 0.92492265 0.68128467 0.6783837
 0.8730842  0.85497606 0.632086   0.57303673 0.5035314  0.67086905
 0.8367648  0.9328443  0.50777096 0.9281217  0.9253347  0.9054351
 0.9332523  0.9243287  0.92005783 0.41732317 0.85350865 0.6904271
 0.917351   0.92433375 0.9095785  0.9352572  0.60602415 0.9171106
 0.9134232  0.48924437 0.4456954  0.54607403 0.6265081  0.92291534
 0.6281615  0.89354706 0.92991215 0.9321674  0.932412   0.6790806
 0.5278822  0.87806225 0.8224786  0.9126086  0.8242397  0.61415946
 0.4943432  0.7889214  0.9310897  0.8842454  0.9314589  0.86143273
 0.91772664 0.9009132  0.92599016 0.92056245 0.536837   0.79114145
 0.5739229  0.8664707  0.9258966  0.92113686 0.92561215 0.4823391
 0.90817577 0.90063316 0.930145   0.91365707 0.9223087  0.8868688
 0.808537   0.9113227  0.9274503  0.49277374 0.54973704 0.8239897
 0.579455   0.6364296  0.9264662  0.502618   0.92306733 0.92555636
 0.8092766  0.5323983  0.67777795 0.87872976 0.933078   0.46268722
 0.877265   0.47272488 0.4449585  0.62834877 0.4880464  0.9351398
 0.6163379  0.5069374  0.92130375 0.925532   0.9163188  0.8738168
 0.79963547 0.45873547 0.46128964 0.7667034  0.5006873  0.9297332
 0.4915092  0.76112026 0.606897   0.8658582  0.9274517  0.9304248
 0.51274693 0.91992617 0.9102986  0.8861762  0.9296898  0.90490514
 0.879479   0.9123286  0.92791516 0.42005002 0.8887845  0.92133886
 0.71470207 0.88334507 0.7164775  0.9324848  0.50117064 0.92411566
 0.937489   0.68169814 0.67689407 0.88238865 0.5104379  0.9302478
 0.92647564 0.9083608  0.73307854 0.92542136 0.4968906  0.61138284
 0.88338053 0.88652736 0.92140377 0.8771631  0.41885787 0.8921843
 0.5221789  0.9138158  0.9319516  0.7913983  0.8145947  0.6586966
 0.48784983 0.90317166 0.9112324  0.9301698  0.91442055 0.91820955
 0.903517   0.92465144 0.86312693 0.9211675  0.9182692  0.9188924
 0.8894526  0.4912734  0.92923486 0.48673755 0.8396026  0.93441814
 0.66370064 0.44382292 0.6073347  0.7124646  0.9222161  0.9284686
 0.676485   0.713134   0.9092515  0.42085466 0.89232564 0.9163323
 0.46372145 0.9213346  0.92688215 0.4569824  0.7424581  0.6672779
 0.8731548  0.9311646  0.8503544  0.4400682  0.48833686 0.92887986
 0.884697   0.86250675 0.92415863 0.9216556  0.43728635 0.6674387
 0.8566062  0.6099419  0.9231338  0.92440414 0.5497797  0.53261393
 0.84993815 0.9252283  0.929289   0.6466481  0.56754786 0.62774396
 0.8466702  0.48289007 0.92099553 0.44810364 0.90762    0.801268
 0.7211893  0.9195977  0.9345219  0.91805375 0.46541014 0.44280663
 0.9261978  0.9237019  0.91906506 0.8843285  0.9276878  0.5508952
 0.91746515 0.88609713 0.9207428  0.90641606 0.93507046 0.7024807
 0.92366284 0.56659454 0.8928453  0.89698106 0.58654785 0.75510156
 0.9311359  0.4627876  0.4521559  0.8950067  0.8849812  0.9338013
 0.897823   0.9023479  0.927774   0.9072692 ]
video_auc: 0.9564418859649123
label: [1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1
 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1
 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0
 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0
 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0
 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1
 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1
 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0
 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1
 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1
 0 1 1 0 0 0 1 1 0 1 1 1]
===> Test Done!
+ python3 training/test.py --detector_path ./training/config/detector/clip_stan.yaml --test_dataset FaceShifter DeepFakeDetection --weights_path ./logs/training/clip_stan_2025-05-30-23-08-08/test/avg/ckpt_best.pth
/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'groups', 'bias', 'padding_mode', 'device', 'dtype']
spatial_count=0 keep_stride_count=0
batch_size: 32
Skipping video DFD_fake_03_01__talking_angry_couch__JZUXXFRB because it has less than clip_size (8) frames (1).
Skipping video DFD_fake_03_06__walking_down_street_outside_angry__1IXGY2FK because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_05_08__hugging_happy__FBICSP2C because it has less than clip_size (8) frames (7).
Skipping video DFD_fake_07_02__secret_conversation__1JCLEEBQ because it has less than clip_size (8) frames (4).
Skipping video DFD_fake_07_03__secret_conversation__IFSURI9X because it has less than clip_size (8) frames (4).
Skipping video DFD_fake_07_09__secret_conversation__N9CWME71 because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_07_12__secret_conversation__1MSOAVLL because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_07_14__secret_conversation__P9QFO50U because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_07_21__secret_conversation__CRGVSDBI because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_11_21__talking_angry_couch__T7DK03O1 because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_11_21__walking_outside_cafe_disgusted__T7DK03O1 because it has less than clip_size (8) frames (1).
Skipping video DFD_fake_12_06__secret_conversation__F3I4PDYF because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_12_07__secret_conversation__X9X7FAZG because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_12_13__secret_conversation__2TM4IFSF because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_12_15__secret_conversation__N0SRODQD because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_12_20__secret_conversation__B0X1CGG2 because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_12_20__secret_conversation__VD7BCF1U because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_12_21__secret_conversation__J54W3PV1 because it has less than clip_size (8) frames (4).
Skipping video DFD_fake_13_02__secret_conversation__PLNVLO74 because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_13_03__secret_conversation__GBYWJW06 because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_13_12__secret_conversation__6G44TDN4 because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_13_12__secret_conversation__DJ7MF331 because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_13_15__secret_conversation__X0EIIJF7 because it has less than clip_size (8) frames (7).
Skipping video DFD_fake_14_03__walking_down_indoor_hall_disgust__KJ221YN0 because it has less than clip_size (8) frames (1).
Skipping video DFD_fake_14_06__secret_conversation__8U9ULZDT because it has less than clip_size (8) frames (7).
Skipping video DFD_fake_14_18__talking_angry_couch__0YRBHIKG because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_14_20__walk_down_hall_angry__B014BKVO because it has less than clip_size (8) frames (7).
Skipping video DFD_fake_14_21__secret_conversation__N0WM4GSV because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_15_02__walking_outside_cafe_disgusted__HTG660F8 because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_15_04__kitchen_still__46TJ9IOJ because it has less than clip_size (8) frames (1).
Skipping video DFD_fake_16_08__walking_down_indoor_hall_disgust__8Q7JCS95 because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_17_05__hugging_happy__C4VHXZFY because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_17_05__hugging_happy__YTJYYDO9 because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_17_08__hugging_happy__05AN86QA because it has less than clip_size (8) frames (1).
Skipping video DFD_fake_17_16__hugging_happy__S7UMSIQV because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_20_01__secret_conversation__6UBMLXK3 because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_20_01__secret_conversation__FW94AIMJ because it has less than clip_size (8) frames (4).
Skipping video DFD_fake_20_11__secret_conversation__B2H95QXV because it has less than clip_size (8) frames (7).
Skipping video DFD_fake_20_13__secret_conversation__4RU7I77X because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_20_14__secret_conversation__B014BKVO because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_20_14__walking_down_indoor_hall_disgust__R7SPX6OK because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_20_18__walking_down_indoor_hall_disgust__8BMQOD7S because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_20_26__secret_conversation__34DJQS3E because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_21_03__outside_talking_pan_laughing__YCSEBZO4 because it has less than clip_size (8) frames (3).
Skipping video DFD_fake_21_09__kitchen_still__LBQF8ZN1 because it has less than clip_size (8) frames (1).
Skipping video DFD_fake_21_25__walking_and_outside_surprised__NFBISHIN because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_22_24__walking_down_street_outside_angry__XL557XC6 because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_23_24__outside_talking_still_laughing__YR5OVD4S because it has less than clip_size (8) frames (2).
Skipping video DFD_fake_23_24__podium_speech_happy__YR5OVD4S because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_25_02__exit_phone_room__Z7FQ69VP because it has less than clip_size (8) frames (5).
Skipping video DFD_fake_25_06__secret_conversation__3QQBVBYN because it has less than clip_size (8) frames (4).
Skipping video DFD_fake_25_26__secret_conversation__GJT740J9 because it has less than clip_size (8) frames (6).
Skipping video DFD_fake_27_13__walking_and_outside_surprised__A1OSUJE9 because it has less than clip_size (8) frames (2).
Skipping video DFD_real_07__secret_conversation because it has less than clip_size (8) frames (6).
Skipping video DFD_real_17__hugging_happy because it has less than clip_size (8) frames (2).
Skipping video DFD_real_20__secret_conversation because it has less than clip_size (8) frames (4).
clip_path: weights/clip-vit-base-patch16
features_dim: 768

All parameters:
class_embedding shape = (768,)
cls_token shape = (1, 1, 768)
feature_extractor.vision_model.embeddings.patch_embedding.weight shape = (768, 3, 16, 16)
feature_extractor.vision_model.embeddings.position_embedding.weight shape = (197, 768)
feature_extractor.vision_model.pre_layrnorm.weight shape = (768,)
feature_extractor.vision_model.pre_layrnorm.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.0.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.0.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.0.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.1.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.1.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.1.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.2.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.2.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.2.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.3.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.3.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.3.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.4.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.4.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.4.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.5.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.5.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.5.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.6.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.6.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.6.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.7.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.7.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.7.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.8.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.8.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.8.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.9.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.9.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.9.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.10.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.10.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.10.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.11.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.11.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.11.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.post_layernorm.weight shape = (768,)
feature_extractor.vision_model.post_layernorm.bias shape = (768,)
STAN_S_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.0.layer_norm1.weight shape = (768,)
STAN_S_layers.0.layer_norm1.bias shape = (768,)
STAN_S_layers.0.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.0.mlp.fc1.bias shape = (3072,)
STAN_S_layers.0.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.0.mlp.fc2.bias shape = (768,)
STAN_S_layers.0.layer_norm2.weight shape = (768,)
STAN_S_layers.0.layer_norm2.bias shape = (768,)
STAN_S_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.1.layer_norm1.weight shape = (768,)
STAN_S_layers.1.layer_norm1.bias shape = (768,)
STAN_S_layers.1.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.1.mlp.fc1.bias shape = (3072,)
STAN_S_layers.1.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.1.mlp.fc2.bias shape = (768,)
STAN_S_layers.1.layer_norm2.weight shape = (768,)
STAN_S_layers.1.layer_norm2.bias shape = (768,)
STAN_S_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.2.layer_norm1.weight shape = (768,)
STAN_S_layers.2.layer_norm1.bias shape = (768,)
STAN_S_layers.2.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.2.mlp.fc1.bias shape = (3072,)
STAN_S_layers.2.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.2.mlp.fc2.bias shape = (768,)
STAN_S_layers.2.layer_norm2.weight shape = (768,)
STAN_S_layers.2.layer_norm2.bias shape = (768,)
STAN_S_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.3.layer_norm1.weight shape = (768,)
STAN_S_layers.3.layer_norm1.bias shape = (768,)
STAN_S_layers.3.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.3.mlp.fc1.bias shape = (3072,)
STAN_S_layers.3.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.3.mlp.fc2.bias shape = (768,)
STAN_S_layers.3.layer_norm2.weight shape = (768,)
STAN_S_layers.3.layer_norm2.bias shape = (768,)
STAN_T_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.0.layer_norm1.weight shape = (768,)
STAN_T_layers.0.layer_norm1.bias shape = (768,)
STAN_T_layers.0.temporal_fc.weight shape = (768, 768)
STAN_T_layers.0.temporal_fc.bias shape = (768,)
STAN_T_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.1.layer_norm1.weight shape = (768,)
STAN_T_layers.1.layer_norm1.bias shape = (768,)
STAN_T_layers.1.temporal_fc.weight shape = (768, 768)
STAN_T_layers.1.temporal_fc.bias shape = (768,)
STAN_T_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.2.layer_norm1.weight shape = (768,)
STAN_T_layers.2.layer_norm1.bias shape = (768,)
STAN_T_layers.2.temporal_fc.weight shape = (768, 768)
STAN_T_layers.2.temporal_fc.bias shape = (768,)
STAN_T_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.3.layer_norm1.weight shape = (768,)
STAN_T_layers.3.layer_norm1.bias shape = (768,)
STAN_T_layers.3.temporal_fc.weight shape = (768, 768)
STAN_T_layers.3.temporal_fc.bias shape = (768,)
time_embed.weight shape = (64, 768)
model.linear.weight shape = (2, 768)
model.linear.bias shape = (2,)

ğŸ”¥ Trainable parameters:
class_embedding shape = (768,)
cls_token shape = (1, 1, 768)
feature_extractor.vision_model.pre_layrnorm.weight shape = (768,)
feature_extractor.vision_model.pre_layrnorm.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.post_layernorm.weight shape = (768,)
feature_extractor.vision_model.post_layernorm.bias shape = (768,)
STAN_S_layers.0.layer_norm1.weight shape = (768,)
STAN_S_layers.0.layer_norm1.bias shape = (768,)
STAN_S_layers.0.layer_norm2.weight shape = (768,)
STAN_S_layers.0.layer_norm2.bias shape = (768,)
STAN_S_layers.1.layer_norm1.weight shape = (768,)
STAN_S_layers.1.layer_norm1.bias shape = (768,)
STAN_S_layers.1.layer_norm2.weight shape = (768,)
STAN_S_layers.1.layer_norm2.bias shape = (768,)
STAN_S_layers.2.layer_norm1.weight shape = (768,)
STAN_S_layers.2.layer_norm1.bias shape = (768,)
STAN_S_layers.2.layer_norm2.weight shape = (768,)
STAN_S_layers.2.layer_norm2.bias shape = (768,)
STAN_S_layers.3.layer_norm1.weight shape = (768,)
STAN_S_layers.3.layer_norm1.bias shape = (768,)
STAN_S_layers.3.layer_norm2.weight shape = (768,)
STAN_S_layers.3.layer_norm2.bias shape = (768,)
STAN_T_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.0.layer_norm1.weight shape = (768,)
STAN_T_layers.0.layer_norm1.bias shape = (768,)
STAN_T_layers.0.temporal_fc.weight shape = (768, 768)
STAN_T_layers.0.temporal_fc.bias shape = (768,)
STAN_T_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.1.layer_norm1.weight shape = (768,)
STAN_T_layers.1.layer_norm1.bias shape = (768,)
STAN_T_layers.1.temporal_fc.weight shape = (768, 768)
STAN_T_layers.1.temporal_fc.bias shape = (768,)
STAN_T_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.2.layer_norm1.weight shape = (768,)
STAN_T_layers.2.layer_norm1.bias shape = (768,)
STAN_T_layers.2.temporal_fc.weight shape = (768, 768)
STAN_T_layers.2.temporal_fc.bias shape = (768,)
STAN_T_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.3.layer_norm1.weight shape = (768,)
STAN_T_layers.3.layer_norm1.bias shape = (768,)
STAN_T_layers.3.temporal_fc.weight shape = (768, 768)
STAN_T_layers.3.temporal_fc.bias shape = (768,)
time_embed.weight shape = (64, 768)
model.linear.weight shape = (2, 768)
model.linear.bias shape = (2,)
Total parameters: 126020354, trainable: 11922434, %: 9.4607
===> Load checkpoint done!

Dataset: FaceShifter
Number of images: 1118
Number of labels: 1118
test images: 1118

  0%|          | 0/35 [00:00<?, ?it/s]
  3%|â–         | 1/35 [00:05<03:20,  5.90s/it]
  6%|â–Œ         | 2/35 [00:07<01:48,  3.28s/it]
  9%|â–Š         | 3/35 [00:08<01:18,  2.44s/it]
 11%|â–ˆâ–        | 4/35 [00:10<01:03,  2.05s/it]
 14%|â–ˆâ–        | 5/35 [00:11<00:54,  1.83s/it]
 17%|â–ˆâ–‹        | 6/35 [00:13<00:49,  1.70s/it]
 20%|â–ˆâ–ˆ        | 7/35 [00:14<00:45,  1.62s/it]
 23%|â–ˆâ–ˆâ–       | 8/35 [00:16<00:42,  1.56s/it]
 26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:17<00:39,  1.53s/it]
 29%|â–ˆâ–ˆâ–Š       | 10/35 [00:18<00:37,  1.50s/it]
 31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:20<00:35,  1.49s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:21<00:33,  1.47s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:23<00:32,  1.46s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:24<00:30,  1.46s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/35 [00:26<00:29,  1.46s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:27<00:27,  1.46s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:29<00:26,  1.45s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:30<00:24,  1.45s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:31<00:23,  1.45s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:33<00:21,  1.45s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:34<00:20,  1.45s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22/35 [00:36<00:18,  1.45s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:37<00:17,  1.45s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:39<00:15,  1.45s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [00:40<00:14,  1.45s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:42<00:13,  1.45s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [00:43<00:11,  1.45s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:44<00:10,  1.45s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 29/35 [00:46<00:08,  1.45s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:47<00:07,  1.45s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:49<00:05,  1.45s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:50<00:04,  1.46s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:52<00:02,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:53<00:01,  1.46s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:56<00:00,  1.78s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:56<00:00,  1.61s/it]

Detailed metrics for FaceShifter:
True Negatives (Real classified as Real): 247
False Positives (Real classified as Fake): 312
False Negatives (Fake classified as Real): 38
True Positives (Fake classified as Fake): 521
Accuracy: 0.6869
Precision: 0.6255
Recall: 0.9320
F1 Score: 0.7486

Predictions shape: (1118,)
Labels shape: (1118,)
Number of processed images: 1118
Number of processed image names: 1118
len(y_pred): 1118
len(y_true): 1118

dataset: FaceShifter
acc: 0.6869409660107334
auc: 0.8290968090859925
eer: 0.24329159212880144
ap: 0.8273220592252218
pred: [0.5613967  0.55148786 0.924219   ... 0.6186234  0.44729835 0.9014211 ]
video_auc: 0.8290968090859925
label: [1 0 1 ... 0 0 1]

Dataset: DeepFakeDetection
Number of images: 11460
Number of labels: 11460
test images: 11460

  0%|          | 0/359 [00:00<?, ?it/s]
  0%|          | 1/359 [00:04<29:16,  4.91s/it]
  1%|          | 2/359 [00:06<17:31,  2.94s/it]
  1%|          | 3/359 [00:07<13:28,  2.27s/it]
  1%|          | 4/359 [00:09<11:33,  1.95s/it]
  1%|â–         | 5/359 [00:10<10:26,  1.77s/it]
  2%|â–         | 6/359 [00:12<09:45,  1.66s/it]
  2%|â–         | 7/359 [00:13<09:20,  1.59s/it]
  2%|â–         | 8/359 [00:15<09:03,  1.55s/it]
  3%|â–         | 9/359 [00:16<08:51,  1.52s/it]
  3%|â–         | 10/359 [00:18<08:41,  1.49s/it]
  3%|â–         | 11/359 [00:19<08:35,  1.48s/it]
  3%|â–         | 12/359 [00:21<08:30,  1.47s/it]
  4%|â–         | 13/359 [00:22<08:27,  1.47s/it]
  4%|â–         | 14/359 [00:23<08:24,  1.46s/it]
  4%|â–         | 15/359 [00:25<08:21,  1.46s/it]
  4%|â–         | 16/359 [00:26<08:19,  1.46s/it]
  5%|â–         | 17/359 [00:28<08:17,  1.46s/it]
  5%|â–Œ         | 18/359 [00:29<08:16,  1.46s/it]
  5%|â–Œ         | 19/359 [00:31<08:14,  1.45s/it]
  6%|â–Œ         | 20/359 [00:32<08:12,  1.45s/it]
  6%|â–Œ         | 21/359 [00:34<08:11,  1.45s/it]
  6%|â–Œ         | 22/359 [00:35<08:10,  1.45s/it]
  6%|â–‹         | 23/359 [00:36<08:09,  1.46s/it]
  7%|â–‹         | 24/359 [00:38<08:08,  1.46s/it]
  7%|â–‹         | 25/359 [00:39<08:10,  1.47s/it]
  7%|â–‹         | 26/359 [00:41<08:07,  1.47s/it]
  8%|â–Š         | 27/359 [00:42<08:07,  1.47s/it]
  8%|â–Š         | 28/359 [00:44<08:06,  1.47s/it]
  8%|â–Š         | 29/359 [00:45<08:05,  1.47s/it]
  8%|â–Š         | 30/359 [00:47<08:02,  1.47s/it]
  9%|â–Š         | 31/359 [00:48<07:59,  1.46s/it]
  9%|â–‰         | 32/359 [00:50<07:57,  1.46s/it]
  9%|â–‰         | 33/359 [00:51<07:55,  1.46s/it]
  9%|â–‰         | 34/359 [00:53<07:55,  1.46s/it]
 10%|â–‰         | 35/359 [00:54<07:53,  1.46s/it]
 10%|â–ˆ         | 36/359 [00:56<07:51,  1.46s/it]
 10%|â–ˆ         | 37/359 [00:57<07:49,  1.46s/it]
 11%|â–ˆ         | 38/359 [00:58<07:48,  1.46s/it]
 11%|â–ˆ         | 39/359 [01:00<07:47,  1.46s/it]
 11%|â–ˆ         | 40/359 [01:01<07:45,  1.46s/it]
 11%|â–ˆâ–        | 41/359 [01:03<07:43,  1.46s/it]
 12%|â–ˆâ–        | 42/359 [01:04<07:42,  1.46s/it]
 12%|â–ˆâ–        | 43/359 [01:06<07:41,  1.46s/it]
 12%|â–ˆâ–        | 44/359 [01:07<07:39,  1.46s/it]
 13%|â–ˆâ–        | 45/359 [01:09<07:38,  1.46s/it]
 13%|â–ˆâ–        | 46/359 [01:10<07:37,  1.46s/it]
 13%|â–ˆâ–        | 47/359 [01:12<07:34,  1.46s/it]
 13%|â–ˆâ–        | 48/359 [01:13<07:32,  1.46s/it]
 14%|â–ˆâ–        | 49/359 [01:14<07:31,  1.46s/it]
 14%|â–ˆâ–        | 50/359 [01:16<07:29,  1.46s/it]
 14%|â–ˆâ–        | 51/359 [01:17<07:28,  1.46s/it]
 14%|â–ˆâ–        | 52/359 [01:19<07:26,  1.46s/it]
 15%|â–ˆâ–        | 53/359 [01:20<07:26,  1.46s/it]
 15%|â–ˆâ–Œ        | 54/359 [01:22<07:25,  1.46s/it]
 15%|â–ˆâ–Œ        | 55/359 [01:23<07:25,  1.46s/it]
 16%|â–ˆâ–Œ        | 56/359 [01:25<07:23,  1.46s/it]
 16%|â–ˆâ–Œ        | 57/359 [01:26<07:22,  1.47s/it]
 16%|â–ˆâ–Œ        | 58/359 [01:28<07:21,  1.47s/it]
 16%|â–ˆâ–‹        | 59/359 [01:29<07:19,  1.46s/it]
 17%|â–ˆâ–‹        | 60/359 [01:31<07:18,  1.47s/it]
 17%|â–ˆâ–‹        | 61/359 [01:32<07:16,  1.47s/it]
 17%|â–ˆâ–‹        | 62/359 [01:34<07:14,  1.46s/it]
 18%|â–ˆâ–Š        | 63/359 [01:35<07:13,  1.46s/it]
 18%|â–ˆâ–Š        | 64/359 [01:36<07:11,  1.46s/it]
 18%|â–ˆâ–Š        | 65/359 [01:38<07:11,  1.47s/it]
 18%|â–ˆâ–Š        | 66/359 [01:39<07:09,  1.47s/it]
 19%|â–ˆâ–Š        | 67/359 [01:41<07:08,  1.47s/it]
 19%|â–ˆâ–‰        | 68/359 [01:42<07:06,  1.46s/it]
 19%|â–ˆâ–‰        | 69/359 [01:44<07:05,  1.47s/it]
 19%|â–ˆâ–‰        | 70/359 [01:45<07:04,  1.47s/it]
 20%|â–ˆâ–‰        | 71/359 [01:47<07:03,  1.47s/it]
 20%|â–ˆâ–ˆ        | 72/359 [01:48<07:00,  1.47s/it]
 20%|â–ˆâ–ˆ        | 73/359 [01:50<06:58,  1.46s/it]
 21%|â–ˆâ–ˆ        | 74/359 [01:51<06:56,  1.46s/it]
 21%|â–ˆâ–ˆ        | 75/359 [01:53<06:54,  1.46s/it]
 21%|â–ˆâ–ˆ        | 76/359 [01:54<06:52,  1.46s/it]
 21%|â–ˆâ–ˆâ–       | 77/359 [01:55<06:52,  1.46s/it]
 22%|â–ˆâ–ˆâ–       | 78/359 [01:57<06:50,  1.46s/it]
 22%|â–ˆâ–ˆâ–       | 79/359 [01:58<06:48,  1.46s/it]
 22%|â–ˆâ–ˆâ–       | 80/359 [02:00<06:47,  1.46s/it]
 23%|â–ˆâ–ˆâ–       | 81/359 [02:01<06:45,  1.46s/it]
 23%|â–ˆâ–ˆâ–       | 82/359 [02:03<06:44,  1.46s/it]
 23%|â–ˆâ–ˆâ–       | 83/359 [02:04<06:42,  1.46s/it]
 23%|â–ˆâ–ˆâ–       | 84/359 [02:06<06:41,  1.46s/it]
 24%|â–ˆâ–ˆâ–       | 85/359 [02:07<06:39,  1.46s/it]
 24%|â–ˆâ–ˆâ–       | 86/359 [02:09<06:37,  1.46s/it]
 24%|â–ˆâ–ˆâ–       | 87/359 [02:10<06:37,  1.46s/it]
 25%|â–ˆâ–ˆâ–       | 88/359 [02:12<06:36,  1.46s/it]
 25%|â–ˆâ–ˆâ–       | 89/359 [02:13<06:34,  1.46s/it]
 25%|â–ˆâ–ˆâ–Œ       | 90/359 [02:14<06:33,  1.46s/it]
 25%|â–ˆâ–ˆâ–Œ       | 91/359 [02:16<06:31,  1.46s/it]
 26%|â–ˆâ–ˆâ–Œ       | 92/359 [02:17<06:29,  1.46s/it]
 26%|â–ˆâ–ˆâ–Œ       | 93/359 [02:19<06:28,  1.46s/it]
 26%|â–ˆâ–ˆâ–Œ       | 94/359 [02:20<06:26,  1.46s/it]
 26%|â–ˆâ–ˆâ–‹       | 95/359 [02:22<06:24,  1.46s/it]
 27%|â–ˆâ–ˆâ–‹       | 96/359 [02:23<06:23,  1.46s/it]
 27%|â–ˆâ–ˆâ–‹       | 97/359 [02:25<06:21,  1.46s/it]
 27%|â–ˆâ–ˆâ–‹       | 98/359 [02:26<06:20,  1.46s/it]
 28%|â–ˆâ–ˆâ–Š       | 99/359 [02:28<06:18,  1.46s/it]
 28%|â–ˆâ–ˆâ–Š       | 100/359 [02:29<06:16,  1.46s/it]
 28%|â–ˆâ–ˆâ–Š       | 101/359 [02:30<06:15,  1.46s/it]
 28%|â–ˆâ–ˆâ–Š       | 102/359 [02:32<06:13,  1.46s/it]
 29%|â–ˆâ–ˆâ–Š       | 103/359 [02:33<06:12,  1.46s/it]
 29%|â–ˆâ–ˆâ–‰       | 104/359 [02:35<06:11,  1.46s/it]
 29%|â–ˆâ–ˆâ–‰       | 105/359 [02:36<06:09,  1.46s/it]
 30%|â–ˆâ–ˆâ–‰       | 106/359 [02:38<06:08,  1.46s/it]
 30%|â–ˆâ–ˆâ–‰       | 107/359 [02:39<06:07,  1.46s/it]
 30%|â–ˆâ–ˆâ–ˆ       | 108/359 [02:41<06:05,  1.46s/it]
 30%|â–ˆâ–ˆâ–ˆ       | 109/359 [02:42<06:04,  1.46s/it]
 31%|â–ˆâ–ˆâ–ˆ       | 110/359 [02:44<06:02,  1.46s/it]
 31%|â–ˆâ–ˆâ–ˆ       | 111/359 [02:45<06:01,  1.46s/it]
 31%|â–ˆâ–ˆâ–ˆ       | 112/359 [02:47<06:00,  1.46s/it]
 31%|â–ˆâ–ˆâ–ˆâ–      | 113/359 [02:48<05:59,  1.46s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 114/359 [02:49<05:57,  1.46s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 115/359 [02:51<05:56,  1.46s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 116/359 [02:52<05:55,  1.46s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 117/359 [02:54<05:53,  1.46s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 118/359 [02:55<05:52,  1.46s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 119/359 [02:57<05:50,  1.46s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 120/359 [02:58<05:48,  1.46s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 121/359 [03:00<05:47,  1.46s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 122/359 [03:01<05:45,  1.46s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 123/359 [03:03<05:43,  1.46s/it]
 35%|â–ˆâ–ˆâ–ˆâ–      | 124/359 [03:04<05:42,  1.46s/it]
 35%|â–ˆâ–ˆâ–ˆâ–      | 125/359 [03:05<05:40,  1.46s/it]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 126/359 [03:07<05:39,  1.46s/it]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 127/359 [03:08<05:37,  1.46s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 128/359 [03:10<05:36,  1.46s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/359 [03:11<05:35,  1.46s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/359 [03:13<05:33,  1.46s/it]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 131/359 [03:14<05:32,  1.46s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 132/359 [03:16<05:30,  1.46s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 133/359 [03:17<05:29,  1.46s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/359 [03:19<05:27,  1.46s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 135/359 [03:20<05:26,  1.46s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 136/359 [03:22<05:24,  1.46s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 137/359 [03:23<05:23,  1.46s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/359 [03:24<05:22,  1.46s/it]
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 139/359 [03:26<05:20,  1.46s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 140/359 [03:27<05:19,  1.46s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 141/359 [03:29<05:17,  1.46s/it]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 142/359 [03:30<05:16,  1.46s/it]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 143/359 [03:32<05:14,  1.46s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144/359 [03:33<05:12,  1.46s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 145/359 [03:35<05:11,  1.46s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 146/359 [03:36<05:09,  1.45s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/359 [03:38<05:08,  1.46s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/359 [03:39<05:07,  1.46s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 149/359 [03:40<05:05,  1.46s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/359 [03:42<05:04,  1.46s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/359 [03:43<05:02,  1.46s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/359 [03:45<05:01,  1.46s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/359 [03:46<04:59,  1.46s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/359 [03:48<04:58,  1.46s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155/359 [03:49<04:57,  1.46s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/359 [03:51<04:56,  1.46s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/359 [03:52<04:54,  1.46s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/359 [03:54<04:53,  1.46s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/359 [03:55<04:51,  1.46s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/359 [03:56<04:50,  1.46s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/359 [03:58<04:48,  1.46s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 162/359 [03:59<04:47,  1.46s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 163/359 [04:01<04:46,  1.46s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 164/359 [04:02<04:44,  1.46s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 165/359 [04:04<04:43,  1.46s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/359 [04:05<04:41,  1.46s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 167/359 [04:07<04:40,  1.46s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 168/359 [04:08<04:38,  1.46s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 169/359 [04:10<04:37,  1.46s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/359 [04:11<04:35,  1.46s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 171/359 [04:13<04:33,  1.46s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 172/359 [04:14<04:32,  1.46s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 173/359 [04:15<04:31,  1.46s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 174/359 [04:17<04:29,  1.46s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/359 [04:18<04:28,  1.46s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 176/359 [04:20<04:26,  1.46s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 177/359 [04:21<04:25,  1.46s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 178/359 [04:23<04:23,  1.46s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/359 [04:24<04:22,  1.46s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/359 [04:26<04:20,  1.46s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 181/359 [04:27<04:19,  1.46s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 182/359 [04:29<04:18,  1.46s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 183/359 [04:30<04:16,  1.46s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 184/359 [04:31<04:15,  1.46s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 185/359 [04:33<04:13,  1.46s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 186/359 [04:34<04:12,  1.46s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 187/359 [04:36<04:10,  1.46s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188/359 [04:37<04:09,  1.46s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/359 [04:39<04:07,  1.46s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190/359 [04:40<04:06,  1.46s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191/359 [04:42<04:04,  1.46s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192/359 [04:43<04:03,  1.46s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 193/359 [04:45<04:01,  1.46s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/359 [04:46<04:00,  1.46s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/359 [04:48<03:58,  1.46s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/359 [04:49<03:57,  1.46s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/359 [04:50<03:56,  1.46s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 198/359 [04:52<03:54,  1.46s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 199/359 [04:53<03:53,  1.46s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 200/359 [04:55<03:51,  1.46s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 201/359 [04:56<03:50,  1.46s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 202/359 [04:58<03:48,  1.46s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 203/359 [04:59<03:47,  1.46s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 204/359 [05:01<03:45,  1.46s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 205/359 [05:02<03:44,  1.46s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 206/359 [05:04<03:42,  1.46s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 207/359 [05:05<03:41,  1.46s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 208/359 [05:06<03:40,  1.46s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 209/359 [05:08<03:39,  1.46s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 210/359 [05:09<03:37,  1.46s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 211/359 [05:11<03:36,  1.46s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 212/359 [05:12<03:34,  1.46s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 213/359 [05:14<03:33,  1.46s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 214/359 [05:15<03:31,  1.46s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 215/359 [05:17<03:29,  1.46s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 216/359 [05:18<03:28,  1.46s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 217/359 [05:20<03:26,  1.46s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 218/359 [05:21<03:25,  1.46s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 219/359 [05:23<03:24,  1.46s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 220/359 [05:24<03:22,  1.46s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 221/359 [05:25<03:21,  1.46s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 222/359 [05:27<03:19,  1.46s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223/359 [05:28<03:18,  1.46s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224/359 [05:30<03:16,  1.46s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225/359 [05:31<03:15,  1.46s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226/359 [05:33<03:13,  1.46s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227/359 [05:34<03:12,  1.46s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 228/359 [05:36<03:11,  1.46s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 229/359 [05:37<03:10,  1.46s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/359 [05:39<03:08,  1.46s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/359 [05:40<03:06,  1.46s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/359 [05:41<03:05,  1.46s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/359 [05:43<03:03,  1.46s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 234/359 [05:44<03:02,  1.46s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 235/359 [05:46<03:00,  1.46s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 236/359 [05:47<02:59,  1.46s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 237/359 [05:49<02:57,  1.46s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 238/359 [05:50<02:56,  1.46s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 239/359 [05:52<02:55,  1.46s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 240/359 [05:53<02:53,  1.46s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 241/359 [05:55<02:52,  1.46s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 242/359 [05:56<02:50,  1.46s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 243/359 [05:58<02:49,  1.46s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 244/359 [05:59<02:47,  1.46s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 245/359 [06:00<02:46,  1.46s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 246/359 [06:02<02:44,  1.46s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 247/359 [06:03<02:43,  1.46s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 248/359 [06:05<02:41,  1.46s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 249/359 [06:06<02:40,  1.46s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 250/359 [06:08<02:39,  1.46s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 251/359 [06:09<02:37,  1.46s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 252/359 [06:11<02:35,  1.46s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 253/359 [06:12<02:34,  1.46s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 254/359 [06:14<02:33,  1.46s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 255/359 [06:15<02:31,  1.46s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 256/359 [06:16<02:30,  1.46s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 257/359 [06:18<02:28,  1.46s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258/359 [06:19<02:27,  1.46s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259/359 [06:21<02:25,  1.46s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260/359 [06:22<02:24,  1.46s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261/359 [06:24<02:22,  1.46s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262/359 [06:25<02:21,  1.46s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 263/359 [06:27<02:19,  1.46s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 264/359 [06:28<02:18,  1.46s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 265/359 [06:30<02:16,  1.46s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/359 [06:31<02:15,  1.46s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/359 [06:32<02:13,  1.46s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/359 [06:34<02:12,  1.46s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/359 [06:35<02:11,  1.46s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/359 [06:37<02:09,  1.46s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 271/359 [06:38<02:08,  1.46s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 272/359 [06:40<02:06,  1.46s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 273/359 [06:41<02:05,  1.46s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 274/359 [06:43<02:04,  1.46s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 275/359 [06:44<02:02,  1.46s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 276/359 [06:46<02:01,  1.46s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 277/359 [06:47<01:59,  1.46s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 278/359 [06:49<01:58,  1.46s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 279/359 [06:50<01:56,  1.46s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 280/359 [06:51<01:55,  1.46s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 281/359 [06:53<01:53,  1.46s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 282/359 [06:54<01:52,  1.46s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 283/359 [06:56<01:50,  1.46s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 284/359 [06:57<01:49,  1.46s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 285/359 [06:59<01:47,  1.46s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 286/359 [07:00<01:46,  1.46s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 287/359 [07:02<01:44,  1.46s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 288/359 [07:03<01:43,  1.46s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 289/359 [07:05<01:42,  1.46s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 290/359 [07:06<01:40,  1.46s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 291/359 [07:07<01:39,  1.46s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 292/359 [07:09<01:37,  1.46s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293/359 [07:10<01:36,  1.46s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294/359 [07:12<01:34,  1.46s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295/359 [07:13<01:33,  1.46s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296/359 [07:15<01:31,  1.46s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297/359 [07:16<01:30,  1.46s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 298/359 [07:18<01:28,  1.46s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 299/359 [07:19<01:27,  1.46s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 300/359 [07:21<01:26,  1.46s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 301/359 [07:22<01:24,  1.46s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/359 [07:23<01:23,  1.46s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/359 [07:25<01:21,  1.46s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/359 [07:26<01:20,  1.46s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/359 [07:28<01:18,  1.46s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 306/359 [07:29<01:17,  1.46s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 307/359 [07:31<01:15,  1.46s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 308/359 [07:32<01:14,  1.46s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 309/359 [07:34<01:12,  1.46s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 310/359 [07:35<01:11,  1.46s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 311/359 [07:37<01:10,  1.46s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 312/359 [07:38<01:08,  1.46s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 313/359 [07:40<01:07,  1.46s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 314/359 [07:41<01:05,  1.46s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 315/359 [07:42<01:04,  1.46s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 316/359 [07:44<01:02,  1.46s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 317/359 [07:45<01:01,  1.46s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 318/359 [07:47<00:59,  1.46s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 319/359 [07:48<00:58,  1.46s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 320/359 [07:50<00:56,  1.46s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 321/359 [07:51<00:55,  1.46s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 322/359 [07:53<00:54,  1.46s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 323/359 [07:54<00:52,  1.46s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 324/359 [07:56<00:51,  1.46s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 325/359 [07:57<00:49,  1.46s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 326/359 [07:58<00:48,  1.46s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 327/359 [08:00<00:46,  1.46s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328/359 [08:01<00:45,  1.46s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329/359 [08:03<00:43,  1.46s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330/359 [08:04<00:42,  1.46s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331/359 [08:06<00:40,  1.46s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332/359 [08:07<00:39,  1.45s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 333/359 [08:09<00:37,  1.45s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 334/359 [08:10<00:36,  1.46s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/359 [08:12<00:34,  1.45s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/359 [08:13<00:33,  1.45s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 337/359 [08:14<00:32,  1.45s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/359 [08:16<00:30,  1.45s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/359 [08:17<00:29,  1.46s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/359 [08:19<00:27,  1.46s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/359 [08:20<00:26,  1.46s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 342/359 [08:22<00:24,  1.46s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 343/359 [08:23<00:23,  1.46s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 344/359 [08:25<00:21,  1.46s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 345/359 [08:26<00:20,  1.46s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 346/359 [08:28<00:18,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 347/359 [08:29<00:17,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 348/359 [08:31<00:16,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 349/359 [08:32<00:14,  1.46s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 350/359 [08:33<00:13,  1.46s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 351/359 [08:35<00:11,  1.46s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 352/359 [08:36<00:10,  1.46s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 353/359 [08:38<00:08,  1.46s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 354/359 [08:39<00:07,  1.46s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 355/359 [08:41<00:05,  1.46s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 356/359 [08:42<00:04,  1.46s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 357/359 [08:44<00:02,  1.46s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 358/359 [08:45<00:01,  1.46s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 359/359 [08:45<00:00,  1.14s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 359/359 [08:46<00:00,  1.47s/it]

Detailed metrics for DeepFakeDetection:
True Negatives (Real classified as Real): 488
False Positives (Real classified as Fake): 693
False Negatives (Fake classified as Real): 203
True Positives (Fake classified as Fake): 10076
Accuracy: 0.9218
Precision: 0.9356
Recall: 0.9803
F1 Score: 0.9574

Predictions shape: (11460,)
Labels shape: (11460,)
Number of processed images: 11460
Number of processed image names: 11460
len(y_pred): 11460
len(y_true): 11460

dataset: DeepFakeDetection
acc: 0.9218150087260035
auc: 0.9277412519248116
eer: 0.15410668924640136
ap: 0.9909402351808921
pred: [0.7674624  0.81848705 0.9313117  ... 0.93785036 0.9196439  0.53889114]
video_auc: 0.9277412519248116
label: [1 1 1 ... 1 1 1]
===> Test Done!
+ python3 training/test.py --detector_path ./training/config/detector/clip_stan.yaml --test_dataset FaceForensics++ --weights_path ./logs/training/clip_stan_2025-05-30-23-08-08/test/avg/ckpt_best.pth
/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'groups', 'bias', 'padding_mode', 'device', 'dtype']
spatial_count=0 keep_stride_count=0
batch_size: 32
clip_path: weights/clip-vit-base-patch16
features_dim: 768

All parameters:
class_embedding shape = (768,)
cls_token shape = (1, 1, 768)
feature_extractor.vision_model.embeddings.patch_embedding.weight shape = (768, 3, 16, 16)
feature_extractor.vision_model.embeddings.position_embedding.weight shape = (197, 768)
feature_extractor.vision_model.pre_layrnorm.weight shape = (768,)
feature_extractor.vision_model.pre_layrnorm.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.0.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.0.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.0.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.1.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.1.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.1.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.1.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.2.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.2.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.2.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.2.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.3.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.3.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.3.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.4.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.4.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.4.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.4.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.5.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.5.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.5.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.5.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.6.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.6.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.6.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.7.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.7.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.7.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.8.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.8.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.8.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.9.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.9.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.9.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.9.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.10.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.10.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.10.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.10.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.q_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.q_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.self_attn.out_proj.weight shape = (768, 768)
feature_extractor.vision_model.encoder.layers.11.self_attn.out_proj.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.mlp.fc1.weight shape = (3072, 768)
feature_extractor.vision_model.encoder.layers.11.mlp.fc1.bias shape = (3072,)
feature_extractor.vision_model.encoder.layers.11.mlp.fc2.weight shape = (768, 3072)
feature_extractor.vision_model.encoder.layers.11.mlp.fc2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.post_layernorm.weight shape = (768,)
feature_extractor.vision_model.post_layernorm.bias shape = (768,)
STAN_S_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.0.layer_norm1.weight shape = (768,)
STAN_S_layers.0.layer_norm1.bias shape = (768,)
STAN_S_layers.0.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.0.mlp.fc1.bias shape = (3072,)
STAN_S_layers.0.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.0.mlp.fc2.bias shape = (768,)
STAN_S_layers.0.layer_norm2.weight shape = (768,)
STAN_S_layers.0.layer_norm2.bias shape = (768,)
STAN_S_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.1.layer_norm1.weight shape = (768,)
STAN_S_layers.1.layer_norm1.bias shape = (768,)
STAN_S_layers.1.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.1.mlp.fc1.bias shape = (3072,)
STAN_S_layers.1.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.1.mlp.fc2.bias shape = (768,)
STAN_S_layers.1.layer_norm2.weight shape = (768,)
STAN_S_layers.1.layer_norm2.bias shape = (768,)
STAN_S_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.2.layer_norm1.weight shape = (768,)
STAN_S_layers.2.layer_norm1.bias shape = (768,)
STAN_S_layers.2.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.2.mlp.fc1.bias shape = (3072,)
STAN_S_layers.2.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.2.mlp.fc2.bias shape = (768,)
STAN_S_layers.2.layer_norm2.weight shape = (768,)
STAN_S_layers.2.layer_norm2.bias shape = (768,)
STAN_S_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_S_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_S_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_S_layers.3.layer_norm1.weight shape = (768,)
STAN_S_layers.3.layer_norm1.bias shape = (768,)
STAN_S_layers.3.mlp.fc1.weight shape = (3072, 768)
STAN_S_layers.3.mlp.fc1.bias shape = (3072,)
STAN_S_layers.3.mlp.fc2.weight shape = (768, 3072)
STAN_S_layers.3.mlp.fc2.bias shape = (768,)
STAN_S_layers.3.layer_norm2.weight shape = (768,)
STAN_S_layers.3.layer_norm2.bias shape = (768,)
STAN_T_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.0.layer_norm1.weight shape = (768,)
STAN_T_layers.0.layer_norm1.bias shape = (768,)
STAN_T_layers.0.temporal_fc.weight shape = (768, 768)
STAN_T_layers.0.temporal_fc.bias shape = (768,)
STAN_T_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.1.layer_norm1.weight shape = (768,)
STAN_T_layers.1.layer_norm1.bias shape = (768,)
STAN_T_layers.1.temporal_fc.weight shape = (768, 768)
STAN_T_layers.1.temporal_fc.bias shape = (768,)
STAN_T_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.2.layer_norm1.weight shape = (768,)
STAN_T_layers.2.layer_norm1.bias shape = (768,)
STAN_T_layers.2.temporal_fc.weight shape = (768, 768)
STAN_T_layers.2.temporal_fc.bias shape = (768,)
STAN_T_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.3.layer_norm1.weight shape = (768,)
STAN_T_layers.3.layer_norm1.bias shape = (768,)
STAN_T_layers.3.temporal_fc.weight shape = (768, 768)
STAN_T_layers.3.temporal_fc.bias shape = (768,)
time_embed.weight shape = (64, 768)
model.linear.weight shape = (2, 768)
model.linear.bias shape = (2,)

ğŸ”¥ Trainable parameters:
class_embedding shape = (768,)
cls_token shape = (1, 1, 768)
feature_extractor.vision_model.pre_layrnorm.weight shape = (768,)
feature_extractor.vision_model.pre_layrnorm.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.0.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.1.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.2.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.3.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.4.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.5.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.6.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.7.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.8.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.9.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.10.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm1.bias shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.weight shape = (768,)
feature_extractor.vision_model.encoder.layers.11.layer_norm2.bias shape = (768,)
feature_extractor.vision_model.post_layernorm.weight shape = (768,)
feature_extractor.vision_model.post_layernorm.bias shape = (768,)
STAN_S_layers.0.layer_norm1.weight shape = (768,)
STAN_S_layers.0.layer_norm1.bias shape = (768,)
STAN_S_layers.0.layer_norm2.weight shape = (768,)
STAN_S_layers.0.layer_norm2.bias shape = (768,)
STAN_S_layers.1.layer_norm1.weight shape = (768,)
STAN_S_layers.1.layer_norm1.bias shape = (768,)
STAN_S_layers.1.layer_norm2.weight shape = (768,)
STAN_S_layers.1.layer_norm2.bias shape = (768,)
STAN_S_layers.2.layer_norm1.weight shape = (768,)
STAN_S_layers.2.layer_norm1.bias shape = (768,)
STAN_S_layers.2.layer_norm2.weight shape = (768,)
STAN_S_layers.2.layer_norm2.bias shape = (768,)
STAN_S_layers.3.layer_norm1.weight shape = (768,)
STAN_S_layers.3.layer_norm1.bias shape = (768,)
STAN_S_layers.3.layer_norm2.weight shape = (768,)
STAN_S_layers.3.layer_norm2.bias shape = (768,)
STAN_T_layers.0.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.0.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.0.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.0.layer_norm1.weight shape = (768,)
STAN_T_layers.0.layer_norm1.bias shape = (768,)
STAN_T_layers.0.temporal_fc.weight shape = (768, 768)
STAN_T_layers.0.temporal_fc.bias shape = (768,)
STAN_T_layers.1.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.1.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.1.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.1.layer_norm1.weight shape = (768,)
STAN_T_layers.1.layer_norm1.bias shape = (768,)
STAN_T_layers.1.temporal_fc.weight shape = (768, 768)
STAN_T_layers.1.temporal_fc.bias shape = (768,)
STAN_T_layers.2.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.2.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.2.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.2.layer_norm1.weight shape = (768,)
STAN_T_layers.2.layer_norm1.bias shape = (768,)
STAN_T_layers.2.temporal_fc.weight shape = (768, 768)
STAN_T_layers.2.temporal_fc.bias shape = (768,)
STAN_T_layers.3.self_attn.k_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.k_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.v_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.v_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.q_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.q_proj.bias shape = (768,)
STAN_T_layers.3.self_attn.out_proj.weight shape = (768, 768)
STAN_T_layers.3.self_attn.out_proj.bias shape = (768,)
STAN_T_layers.3.layer_norm1.weight shape = (768,)
STAN_T_layers.3.layer_norm1.bias shape = (768,)
STAN_T_layers.3.temporal_fc.weight shape = (768, 768)
STAN_T_layers.3.temporal_fc.bias shape = (768,)
time_embed.weight shape = (64, 768)
model.linear.weight shape = (2, 768)
model.linear.bias shape = (2,)
Total parameters: 126020354, trainable: 11922434, %: 9.4607
===> Load checkpoint done!

Dataset: FaceForensics++
Number of images: 2788
Number of labels: 2788
test images: 2788

  0%|          | 0/88 [00:00<?, ?it/s]
  1%|          | 1/88 [00:06<10:08,  7.00s/it]
  2%|â–         | 2/88 [00:08<05:20,  3.73s/it]
  3%|â–         | 3/88 [00:09<03:48,  2.68s/it]
  5%|â–         | 4/88 [00:11<03:03,  2.19s/it]
  6%|â–Œ         | 5/88 [00:12<02:39,  1.92s/it]
  7%|â–‹         | 6/88 [00:14<02:23,  1.75s/it]
  8%|â–Š         | 7/88 [00:15<02:13,  1.65s/it]
  9%|â–‰         | 8/88 [00:17<02:06,  1.58s/it]
 10%|â–ˆ         | 9/88 [00:18<02:01,  1.54s/it]
 11%|â–ˆâ–        | 10/88 [00:19<01:57,  1.51s/it]
 12%|â–ˆâ–        | 11/88 [00:21<01:54,  1.49s/it]
 14%|â–ˆâ–        | 12/88 [00:22<01:51,  1.47s/it]
 15%|â–ˆâ–        | 13/88 [00:24<01:49,  1.46s/it]
 16%|â–ˆâ–Œ        | 14/88 [00:25<01:47,  1.46s/it]
 17%|â–ˆâ–‹        | 15/88 [00:27<01:45,  1.45s/it]
 18%|â–ˆâ–Š        | 16/88 [00:28<01:44,  1.45s/it]
 19%|â–ˆâ–‰        | 17/88 [00:30<01:42,  1.45s/it]
 20%|â–ˆâ–ˆ        | 18/88 [00:31<01:41,  1.45s/it]
 22%|â–ˆâ–ˆâ–       | 19/88 [00:32<01:39,  1.44s/it]
 23%|â–ˆâ–ˆâ–       | 20/88 [00:34<01:38,  1.44s/it]
 24%|â–ˆâ–ˆâ–       | 21/88 [00:35<01:36,  1.44s/it]
 25%|â–ˆâ–ˆâ–Œ       | 22/88 [00:37<01:35,  1.44s/it]
 26%|â–ˆâ–ˆâ–Œ       | 23/88 [00:38<01:33,  1.44s/it]
 27%|â–ˆâ–ˆâ–‹       | 24/88 [00:40<01:32,  1.44s/it]
 28%|â–ˆâ–ˆâ–Š       | 25/88 [00:41<01:31,  1.45s/it]
 30%|â–ˆâ–ˆâ–‰       | 26/88 [00:43<01:29,  1.45s/it]
 31%|â–ˆâ–ˆâ–ˆ       | 27/88 [00:44<01:28,  1.45s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 28/88 [00:45<01:27,  1.45s/it]
 33%|â–ˆâ–ˆâ–ˆâ–      | 29/88 [00:47<01:25,  1.45s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 30/88 [00:48<01:24,  1.45s/it]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 31/88 [00:50<01:22,  1.45s/it]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 32/88 [00:51<01:20,  1.45s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 33/88 [00:53<01:19,  1.45s/it]
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 34/88 [00:54<01:18,  1.45s/it]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 35/88 [00:56<01:16,  1.45s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 36/88 [00:57<01:15,  1.45s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 37/88 [00:58<01:13,  1.45s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 38/88 [01:00<01:12,  1.45s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 39/88 [01:01<01:10,  1.44s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 40/88 [01:03<01:09,  1.45s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 41/88 [01:04<01:07,  1.45s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 42/88 [01:06<01:06,  1.45s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 43/88 [01:07<01:05,  1.45s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 44/88 [01:09<01:03,  1.45s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 45/88 [01:10<01:02,  1.45s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 46/88 [01:11<01:00,  1.45s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 47/88 [01:13<00:59,  1.45s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 48/88 [01:14<00:57,  1.45s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 49/88 [01:16<00:56,  1.45s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 50/88 [01:17<00:54,  1.45s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 51/88 [01:19<00:53,  1.45s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 52/88 [01:20<00:52,  1.45s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 53/88 [01:22<00:50,  1.45s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 54/88 [01:23<00:49,  1.45s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 55/88 [01:24<00:47,  1.45s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 56/88 [01:26<00:46,  1.45s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 57/88 [01:27<00:44,  1.45s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 58/88 [01:29<00:43,  1.45s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 59/88 [01:30<00:42,  1.45s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 60/88 [01:32<00:40,  1.45s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 61/88 [01:33<00:39,  1.45s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 62/88 [01:35<00:37,  1.45s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 63/88 [01:36<00:36,  1.45s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 64/88 [01:38<00:34,  1.45s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 65/88 [01:39<00:33,  1.45s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 66/88 [01:40<00:31,  1.45s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 67/88 [01:42<00:30,  1.45s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 68/88 [01:43<00:29,  1.45s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 69/88 [01:45<00:27,  1.45s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 70/88 [01:46<00:26,  1.45s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 71/88 [01:48<00:24,  1.45s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 72/88 [01:49<00:23,  1.45s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 73/88 [01:51<00:21,  1.45s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 74/88 [01:52<00:20,  1.45s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 75/88 [01:53<00:18,  1.45s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 76/88 [01:55<00:17,  1.45s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 77/88 [01:56<00:15,  1.45s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 78/88 [01:58<00:14,  1.45s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 79/88 [01:59<00:13,  1.45s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 80/88 [02:01<00:11,  1.45s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 81/88 [02:02<00:10,  1.45s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 82/88 [02:04<00:08,  1.45s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 83/88 [02:05<00:07,  1.45s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 84/88 [02:07<00:05,  1.45s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 85/88 [02:08<00:04,  1.45s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 86/88 [02:09<00:02,  1.45s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 87/88 [02:11<00:01,  1.45s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [02:11<00:00,  1.13s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [02:12<00:00,  1.50s/it]

Detailed metrics for FaceForensics++:
True Negatives (Real classified as Real): 247
False Positives (Real classified as Fake): 312
False Negatives (Fake classified as Real): 61
True Positives (Fake classified as Fake): 2168
Accuracy: 0.8662
Precision: 0.8742
Recall: 0.9726
F1 Score: 0.9208

Predictions shape: (2788,)
Labels shape: (2788,)
Number of processed images: 2788
Number of processed image names: 2788
len(y_pred): 2788
len(y_true): 2788

dataset: FaceForensics++
acc: 0.8662123385939742
auc: 0.9339335688047697
eer: 0.13774597495527727
ap: 0.9832339085021449
pred: [0.94352466 0.42227957 0.9232749  ... 0.9136926  0.9200884  0.53733486]
video_auc: 0.9339335688047697
label: [1 0 1 ... 1 1 0]
===> Test Done!