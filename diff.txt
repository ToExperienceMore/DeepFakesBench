Found 146 matching output files to compare

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.0.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 15.279190
Mean difference: 3.104684
Min value 1: -9.769507
Max value 1: 8.242389
Min value 2: -18.588667
Max value 2: 6.236896

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.0.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 7.767500
Mean difference: 0.661886
Min value 1: -7.953292
Max value 1: 8.503649
Min value 2: -2.112851
Max value 2: 1.511023

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.756180
Mean difference: 0.316776
Min value 1: -4.482390
Max value 1: 3.574755
Min value 2: -1.828675
Max value 2: 1.600097

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.420809
Mean difference: 0.615503
Min value 1: -7.953730
Max value 1: 7.585744
Min value 2: -4.521523
Max value 2: 6.039953

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 8.061687
Mean difference: 0.232678
Min value 1: -6.933259
Max value 1: 8.292657
Min value 2: -4.216019
Max value 2: 3.578226

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.1.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.1.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 7.378357
Mean difference: 1.556426
Min value 1: -6.992056
Max value 1: 6.350756
Min value 2: -8.763687
Max value 2: 3.374590

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 8.780250
Mean difference: 0.943221
Min value 1: -8.404838
Max value 1: 8.139021
Min value 2: -4.425328
Max value 2: 4.355653

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.031421
Mean difference: 0.404198
Min value 1: -5.347488
Max value 1: 4.235181
Min value 2: -2.365980
Max value 2: 2.565804

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.10.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.10.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 6.513488
Mean difference: 1.195235
Min value 1: -3.102194
Max value 1: 1.861863
Min value 2: -6.767261
Max value 2: 5.283452

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 6.243695
Mean difference: 0.814491
Min value 1: -2.941713
Max value 1: 3.085794
Min value 2: -5.844859
Max value 2: 6.030900

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.11.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.11.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 58.343506
Mean difference: 0.149520
Min value 1: -0.941074
Max value 1: 1.463974
Min value 2: -14.914629
Max value 2: 58.234093

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 6.226132
Mean difference: 0.790758
Min value 1: -2.229459
Max value 1: 2.202489
Min value 2: -5.420062
Max value 2: 6.842552

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.664699
Mean difference: 0.550014
Min value 1: -1.845238
Max value 1: 1.861227
Min value 2: -5.133042
Max value 2: 3.961559

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.12.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.12.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 61.884979
Mean difference: 1.208789
Min value 1: -7.730385
Max value 1: 2.536534
Min value 2: -29.171762
Max value 2: 57.353775

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.12.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.12.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 128.429855
Mean difference: 0.150737
Min value 1: -1.292414
Max value 1: 3.101451
Min value 2: -17.214901
Max value 2: 128.476852

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.12.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 6.501614
Mean difference: 0.794394
Min value 1: -2.682308
Max value 1: 2.691646
Min value 2: -5.575944
Max value 2: 6.804039

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.13.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.13.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.263772
Mean difference: 0.146491
Min value 1: -1.591260
Max value 1: 1.647891
Min value 2: -1.264745
Max value 2: 1.901650

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.13.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 6.387677
Mean difference: 0.830131
Min value 1: -2.596846
Max value 1: 2.771908
Min value 2: -5.950967
Max value 2: 6.008791

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.13.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 0.773924
Mean difference: 0.126187
Min value 1: -1.058871
Max value 1: 0.998441
Min value 2: -1.597605
Max value 2: 1.444358

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.14.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.14.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.707374
Mean difference: 0.163472
Min value 1: -0.927177
Max value 1: 1.318379
Min value 2: -1.131505
Max value 2: 1.599072

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.14.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.585471
Mean difference: 0.786712
Min value 1: -3.631786
Max value 1: 3.370297
Min value 2: -5.262484
Max value 2: 5.839667

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.14.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.400603
Mean difference: 0.543604
Min value 1: -2.000345
Max value 1: 1.854497
Min value 2: -3.148305
Max value 2: 2.623318

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.15.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.15.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 6.736736
Mean difference: 1.389626
Min value 1: -3.021668
Max value 1: 2.092162
Min value 2: -7.069352
Max value 2: 3.986167

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.15.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.15.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.693317
Mean difference: 0.152709
Min value 1: -1.192003
Max value 1: 0.840538
Min value 2: -0.880718
Max value 2: 1.313728

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.15.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.798191
Mean difference: 0.561646
Min value 1: -1.932613
Max value 1: 2.312252
Min value 2: -2.677010
Max value 2: 2.768156

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.16.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 8.407761
Mean difference: 0.803941
Min value 1: -6.456466
Max value 1: 7.087487
Min value 2: -8.002131
Max value 2: 7.681938

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.16.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.640247
Mean difference: 0.582121
Min value 1: -2.222178
Max value 1: 2.007164
Min value 2: -3.064161
Max value 2: 2.750030

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.17.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.17.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.022231
Mean difference: 0.167366
Min value 1: -0.855080
Max value 1: 1.618875
Min value 2: -0.859795
Max value 2: 1.212659

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.17.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 0.711803
Mean difference: 0.136938
Min value 1: -0.523340
Max value 1: 0.508582
Min value 2: -0.944863
Max value 2: 0.450379

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.18.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.18.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 9.897088
Mean difference: 2.176662
Min value 1: -3.124913
Max value 1: 1.912490
Min value 2: -8.820393
Max value 2: 5.329188

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.18.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.570509
Mean difference: 0.621093
Min value 1: -2.099225
Max value 1: 2.065932
Min value 2: -3.437153
Max value 2: 3.442272

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.19.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 7.992158
Mean difference: 0.840964
Min value 1: -6.110549
Max value 1: 6.866529
Min value 2: -8.523005
Max value 2: 10.652629

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.19.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 0.772032
Mean difference: 0.145475
Min value 1: -0.688222
Max value 1: 0.476530
Min value 2: -1.188979
Max value 2: 0.651499

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.19.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.243190
Mean difference: 0.829436
Min value 1: -4.202857
Max value 1: 4.383185
Min value 2: -7.574273
Max value 2: 6.866483

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.19.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.778235
Mean difference: 0.644694
Min value 1: -2.213650
Max value 1: 2.141229
Min value 2: -3.462892
Max value 2: 3.482736

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 6.348145
Mean difference: 0.755301
Min value 1: -5.134364
Max value 1: 5.114435
Min value 2: -4.317580
Max value 2: 4.418260

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.892079
Mean difference: 0.400576
Min value 1: -3.790542
Max value 1: 3.723122
Min value 2: -3.893495
Max value 2: 3.479633

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.20.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.608704
Mean difference: 0.291031
Min value 1: -1.010742
Max value 1: 1.040318
Min value 2: -3.281244
Max value 2: 4.544032

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.20.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 8.114074
Mean difference: 0.845439
Min value 1: -9.912766
Max value 1: 7.443990
Min value 2: -12.870165
Max value 2: 9.022389

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.20.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.844984
Mean difference: 0.165123
Min value 1: -0.793954
Max value 1: 0.553578
Min value 2: -1.695447
Max value 2: 0.619776

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.20.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.839496
Mean difference: 0.691126
Min value 1: -1.999949
Max value 1: 2.412347
Min value 2: -3.477551
Max value 2: 3.733768

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.21.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.21.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 40.151867
Mean difference: 2.052142
Min value 1: -5.670453
Max value 1: 2.259657
Min value 2: -41.912762
Max value 2: 12.451277

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.21.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 8.249185
Mean difference: 0.818251
Min value 1: -9.802624
Max value 1: 13.254909
Min value 2: -12.280450
Max value 2: 15.951134

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.21.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.731557
Mean difference: 0.715101
Min value 1: -2.390512
Max value 1: 2.369020
Min value 2: -3.913233
Max value 2: 3.670912

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.22.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.22.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.744261
Mean difference: 0.263537
Min value 1: -1.988326
Max value 1: 4.259602
Min value 2: -4.461163
Max value 2: 5.429049

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.22.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.940384
Mean difference: 0.664533
Min value 1: -2.376330
Max value 1: 2.124027
Min value 2: -5.512722
Max value 2: 4.111317

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.23.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.23.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 6.807757
Mean difference: 0.921093
Min value 1: -3.305019
Max value 1: 1.701647
Min value 2: -7.483331
Max value 2: 3.807520

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.23.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 9.990747
Mean difference: 0.762888
Min value 1: -2.241184
Max value 1: 2.120894
Min value 2: -9.051817
Max value 2: 10.754487

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.3.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 2.196154
Mean difference: 0.399966
Min value 1: -2.359548
Max value 1: 1.944879
Min value 2: -1.027307
Max value 2: 0.965144

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 6.794104
Mean difference: 0.933229
Min value 1: -5.137743
Max value 1: 4.067796
Min value 2: -6.372273
Max value 2: 6.263556

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 2.159247
Mean difference: 0.186445
Min value 1: -2.957868
Max value 1: 1.462889
Min value 2: -1.810478
Max value 2: 1.018952

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.561321
Mean difference: 0.375552
Min value 1: -3.020479
Max value 1: 3.335933
Min value 2: -2.733574
Max value 2: 3.005022

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.4.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.4.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 2.512296
Mean difference: 0.266446
Min value 1: -2.730071
Max value 1: 1.694456
Min value 2: -0.857923
Max value 2: 1.103313

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.224128
Mean difference: 0.844472
Min value 1: -4.806455
Max value 1: 5.054500
Min value 2: -5.836559
Max value 2: 4.913405

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 3.921468
Mean difference: 0.396562
Min value 1: -3.170394
Max value 1: 3.055660
Min value 2: -3.080354
Max value 2: 2.378052

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.5.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.142643
Mean difference: 0.174455
Min value 1: -1.165401
Max value 1: 1.209025
Min value 2: -0.892623
Max value 2: 1.901080

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.622452
Mean difference: 0.899878
Min value 1: -3.493543
Max value 1: 3.902350
Min value 2: -4.695929
Max value 2: 4.688051

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.198039
Mean difference: 0.780371
Min value 1: -4.077374
Max value 1: 3.968763
Min value 2: -4.340379
Max value 2: 4.930071

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.182975
Mean difference: 0.438256
Min value 1: -3.390358
Max value 1: 2.800873
Min value 2: -2.524128
Max value 2: 3.314266

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.6.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.6.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.005531
Mean difference: 0.162883
Min value 1: -0.797221
Max value 1: 1.472332
Min value 2: -1.300449
Max value 2: 1.879981

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 6.921270
Mean difference: 1.073549
Min value 1: -4.169567
Max value 1: 3.444528
Min value 2: -5.706786
Max value 2: 5.647094

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 1.790201
Mean difference: 0.174142
Min value 1: -1.269464
Max value 1: 0.678506
Min value 2: -1.753922
Max value 2: 1.109746

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.7.mlp.fc1_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.mlp.fc1_output.pt
Shape 1: torch.Size([1, 257, 4096])
Shape 2: torch.Size([1, 257, 4096])
Max difference: 5.643533
Mean difference: 0.798768
Min value 1: -3.639719
Max value 1: 2.387931
Min value 2: -7.216166
Max value 2: 3.980412

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.7.mlp.fc2_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.mlp.fc2_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 2.063973
Mean difference: 0.167685
Min value 1: -0.843267
Max value 1: 1.702377
Min value 2: -1.179465
Max value 2: 2.060303

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.117990
Mean difference: 0.855744
Min value 1: -3.376178
Max value 1: 2.447165
Min value 2: -4.580307
Max value 2: 4.746207

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 0.997106
Mean difference: 0.152697
Min value 1: -1.061290
Max value 1: 0.578359
Min value 2: -1.216233
Max value 2: 0.736907

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 0.923135
Mean difference: 0.170448
Min value 1: -0.834307
Max value 1: 0.951631
Min value 2: -1.199148
Max value 2: 1.019703

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.183833
Mean difference: 0.555824
Min value 1: -2.433711
Max value 1: 2.299576
Min value 2: -4.565288
Max value 2: 3.095271

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.452724
Mean difference: 0.835833
Min value 1: -2.705767
Max value 1: 2.981895
Min value 2: -5.344859
Max value 2: 4.614758

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 5.205144
Mean difference: 0.773156
Min value 1: -3.003063
Max value 1: 3.415955
Min value 2: -6.836345
Max value 2: 4.891581

Layer comparison:
Name 1: feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj_output.pt
Name 2: image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj_output.pt
Shape 1: torch.Size([1, 257, 1024])
Shape 2: torch.Size([1, 257, 1024])
Max difference: 4.361137
Mean difference: 0.582810
Min value 1: -3.166133
Max value 1: 2.765625
Min value 2: -4.755531
Max value 2: 3.441087

Layer comparison:
Name 1: model.linear_output.pt
Name 2: image_0__forward_module.model.linear_output.pt
Shape 1: torch.Size([1, 2])
Shape 2: torch.Size([1, 2])
Max difference: 1.332279
Mean difference: 1.304767
Min value 1: -0.303292
Max value 1: 0.273723
Min value 2: -1.003533
Max value 2: 1.028987

Summary of significant differences:
feature_extractor.vision_model.encoder.layers.0.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.mlp.fc1_output.pt: 3.104684
feature_extractor.vision_model.encoder.layers.18.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.18.mlp.fc1_output.pt: 2.176662
feature_extractor.vision_model.encoder.layers.21.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.21.mlp.fc1_output.pt: 2.052142
feature_extractor.vision_model.encoder.layers.1.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.1.mlp.fc1_output.pt: 1.556426
feature_extractor.vision_model.encoder.layers.15.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.15.mlp.fc1_output.pt: 1.389626
model.linear_output.pt <-> image_0__forward_module.model.linear_output.pt: 1.304767
feature_extractor.vision_model.encoder.layers.12.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.12.mlp.fc1_output.pt: 1.208789
feature_extractor.vision_model.encoder.layers.10.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.10.mlp.fc1_output.pt: 1.195235
feature_extractor.vision_model.encoder.layers.6.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.6.self_attn.k_proj_output.pt: 1.073549
feature_extractor.vision_model.encoder.layers.1.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.1.self_attn.q_proj_output.pt: 0.943221
feature_extractor.vision_model.encoder.layers.3.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.self_attn.k_proj_output.pt: 0.933229
feature_extractor.vision_model.encoder.layers.23.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.23.mlp.fc1_output.pt: 0.921093
feature_extractor.vision_model.encoder.layers.5.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.self_attn.k_proj_output.pt: 0.899878
feature_extractor.vision_model.encoder.layers.7.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.self_attn.k_proj_output.pt: 0.855744
feature_extractor.vision_model.encoder.layers.20.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.self_attn.k_proj_output.pt: 0.845439
feature_extractor.vision_model.encoder.layers.4.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.4.self_attn.q_proj_output.pt: 0.844472
feature_extractor.vision_model.encoder.layers.19.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.k_proj_output.pt: 0.840964
feature_extractor.vision_model.encoder.layers.9.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.9.self_attn.k_proj_output.pt: 0.835833
feature_extractor.vision_model.encoder.layers.13.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.13.self_attn.k_proj_output.pt: 0.830131
feature_extractor.vision_model.encoder.layers.19.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.q_proj_output.pt: 0.829436
feature_extractor.vision_model.encoder.layers.21.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.21.self_attn.k_proj_output.pt: 0.818251
feature_extractor.vision_model.encoder.layers.10.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.10.self_attn.k_proj_output.pt: 0.814491
feature_extractor.vision_model.encoder.layers.16.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.16.self_attn.k_proj_output.pt: 0.803941
feature_extractor.vision_model.encoder.layers.7.mlp.fc1_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.mlp.fc1_output.pt: 0.798768
feature_extractor.vision_model.encoder.layers.12.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.12.self_attn.k_proj_output.pt: 0.794394
feature_extractor.vision_model.encoder.layers.11.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.11.self_attn.k_proj_output.pt: 0.790758
feature_extractor.vision_model.encoder.layers.14.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.14.self_attn.q_proj_output.pt: 0.786712
feature_extractor.vision_model.encoder.layers.5.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.self_attn.q_proj_output.pt: 0.780371
feature_extractor.vision_model.encoder.layers.9.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.9.self_attn.q_proj_output.pt: 0.773156
feature_extractor.vision_model.encoder.layers.23.self_attn.k_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.23.self_attn.k_proj_output.pt: 0.762888
feature_extractor.vision_model.encoder.layers.2.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.2.self_attn.q_proj_output.pt: 0.755301
feature_extractor.vision_model.encoder.layers.21.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.21.self_attn.v_proj_output.pt: 0.715101
feature_extractor.vision_model.encoder.layers.20.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.self_attn.v_proj_output.pt: 0.691126
feature_extractor.vision_model.encoder.layers.22.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.22.self_attn.v_proj_output.pt: 0.664533
feature_extractor.vision_model.encoder.layers.0.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.mlp.fc2_output.pt: 0.661886
feature_extractor.vision_model.encoder.layers.19.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.v_proj_output.pt: 0.644694
feature_extractor.vision_model.encoder.layers.18.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.18.self_attn.v_proj_output.pt: 0.621093
feature_extractor.vision_model.encoder.layers.0.self_attn.q_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.self_attn.q_proj_output.pt: 0.615503
feature_extractor.vision_model.encoder.layers.9.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.9.self_attn.v_proj_output.pt: 0.582810
feature_extractor.vision_model.encoder.layers.16.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.16.self_attn.v_proj_output.pt: 0.582121
feature_extractor.vision_model.encoder.layers.15.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.15.self_attn.v_proj_output.pt: 0.561646
feature_extractor.vision_model.encoder.layers.8.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.8.self_attn.v_proj_output.pt: 0.555824
feature_extractor.vision_model.encoder.layers.11.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.11.self_attn.v_proj_output.pt: 0.550014
feature_extractor.vision_model.encoder.layers.14.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.14.self_attn.v_proj_output.pt: 0.543604
feature_extractor.vision_model.encoder.layers.5.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.self_attn.v_proj_output.pt: 0.438256
feature_extractor.vision_model.encoder.layers.1.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.1.self_attn.v_proj_output.pt: 0.404198
feature_extractor.vision_model.encoder.layers.2.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.2.self_attn.v_proj_output.pt: 0.400576
feature_extractor.vision_model.encoder.layers.3.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.mlp.fc2_output.pt: 0.399966
feature_extractor.vision_model.encoder.layers.4.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.4.self_attn.v_proj_output.pt: 0.396562
feature_extractor.vision_model.encoder.layers.3.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.self_attn.v_proj_output.pt: 0.375552
feature_extractor.vision_model.encoder.layers.0.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.self_attn.out_proj_output.pt: 0.316776
feature_extractor.vision_model.encoder.layers.20.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.mlp.fc2_output.pt: 0.291031
feature_extractor.vision_model.encoder.layers.4.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.4.mlp.fc2_output.pt: 0.266446
feature_extractor.vision_model.encoder.layers.22.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.22.mlp.fc2_output.pt: 0.263537
feature_extractor.vision_model.encoder.layers.0.self_attn.v_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.0.self_attn.v_proj_output.pt: 0.232678
feature_extractor.vision_model.encoder.layers.3.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.3.self_attn.out_proj_output.pt: 0.186445
feature_extractor.vision_model.encoder.layers.5.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.5.mlp.fc2_output.pt: 0.174455
feature_extractor.vision_model.encoder.layers.6.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.6.self_attn.out_proj_output.pt: 0.174142
feature_extractor.vision_model.encoder.layers.8.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.8.self_attn.out_proj_output.pt: 0.170448
feature_extractor.vision_model.encoder.layers.7.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.mlp.fc2_output.pt: 0.167685
feature_extractor.vision_model.encoder.layers.17.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.17.mlp.fc2_output.pt: 0.167366
feature_extractor.vision_model.encoder.layers.20.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.20.self_attn.out_proj_output.pt: 0.165123
feature_extractor.vision_model.encoder.layers.14.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.14.mlp.fc2_output.pt: 0.163472
feature_extractor.vision_model.encoder.layers.6.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.6.mlp.fc2_output.pt: 0.162883
feature_extractor.vision_model.encoder.layers.15.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.15.mlp.fc2_output.pt: 0.152709
feature_extractor.vision_model.encoder.layers.7.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.7.self_attn.out_proj_output.pt: 0.152697
feature_extractor.vision_model.encoder.layers.12.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.12.mlp.fc2_output.pt: 0.150737
feature_extractor.vision_model.encoder.layers.11.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.11.mlp.fc2_output.pt: 0.149520
feature_extractor.vision_model.encoder.layers.13.mlp.fc2_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.13.mlp.fc2_output.pt: 0.146491
feature_extractor.vision_model.encoder.layers.19.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.19.self_attn.out_proj_output.pt: 0.145475
feature_extractor.vision_model.encoder.layers.17.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.17.self_attn.out_proj_output.pt: 0.136938
feature_extractor.vision_model.encoder.layers.13.self_attn.out_proj_output.pt <-> image_0__forward_module.feature_extractor.base_model.model.vision_model.encoder.layers.13.self_attn.out_proj_output.pt: 0.126187
