/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
2025-05-28 03:10:27,599 - INFO - Save log to ./logs/training/clip_enhanced_2025-05-28-03-10-27
2025-05-28 03:10:27,599 - INFO - --------------- Configuration ---------------
2025-05-28 03:10:27,599 - INFO - Parameters: 
name: clip_enhanced
type: clip
model_name: clip_enhanced
backbone: ViT-L/14
pretrained: True
freeze_backbone: False
use_peft: True
peft_type: ln_tuning
use_metric_learning: True
metric_margin: 0.3
use_l2_norm: True
temperature: 0.07
use_contrastive_loss: True
manualSeed: 1024
cuda: True
cudnn: True
log_dir: ./logs/training/
save_ckpt: True
save_feat: True
all_dataset: ['FaceForensics++', 'FF-F2F', 'FF-DF', 'FF-FS', 'FF-NT', 'FaceShifter', 'DeepFakeDetection', 'Celeb-DF-v1', 'Celeb-DF-v2', 'DFDCP', 'DFDC', 'DeeperForensics-1.0', 'UADFV']
train_dataset: ['FaceForensics++']
test_dataset: ['DFDC']
compression: c23
frame_num: {'train': 32, 'test': 32}
with_mask: False
with_landmark: False
resolution: 224
train_batchSize: 32
test_batchSize: 32
workers: 24
classifier: {'type': 'linear', 'in_features': 768, 'out_features': 2, 'bias': True}
optimizer: {'type': 'AdamW', 'lr': '8e-5', 'weight_decay': 0.0, 'betas': [0.9, 0.999]}
use_amp: True
amp_dtype: bf16
amp_level: O1
lr_scheduler: cosine
min_lr: 5e-5
nEpochs: 10
start_epoch: 0
save_epoch: 1
rec_iter: 100
loss: {'type': 'combined', 'ce_weight': 1.0, 'label_smoothing': 0.0, 'bce_labels': 0.0, 'uniformity': 0.0, 'alignment_labels': 0.0}
data: {'img_size': 224, 'batch_size': 128, 'num_workers': 12, 'use_face_detection': True}
use_data_augmentation: True
data_aug: {'flip_prob': 0.5, 'rotate_prob': 0.5, 'rotate_limit': [-10, 10], 'blur_prob': 0.5, 'blur_limit': [3, 7], 'brightness_prob': 0.5, 'brightness_limit': [-0.1, 0.1], 'contrast_limit': [-0.1, 0.1], 'quality_lower': 40, 'quality_upper': 100, 'random_resized_crop': {'size': 224, 'scale': [0.8, 1.0]}, 'color_jitter': {'brightness': 0.2, 'contrast': 0.2, 'saturation': 0.2, 'hue': 0.1}, 'random_erasing': {'p': 0.2}}
mean: [0.48145466, 0.4578275, 0.40821073]
std: [0.26862954, 0.26130258, 0.27577711]
metric_scoring: auc
loss_func: cross_entropy
mode: train
lmdb: False
dry_run: False
rgb_dir: ./datasets/rgb
lmdb_dir: ./datasets/lmdb
dataset_json_folder: ./preprocessing/dataset_json
SWA: False
save_avg: True
label_dict: {'DFD_fake': 1, 'DFD_real': 0, 'FF-SH': 1, 'FF-F2F': 1, 'FF-DF': 1, 'FF-FS': 1, 'FF-NT': 1, 'FF-FH': 1, 'FF-real': 0, 'CelebDFv1_real': 0, 'CelebDFv1_fake': 1, 'CelebDFv2_real': 0, 'CelebDFv2_fake': 1, 'DFDCP_Real': 0, 'DFDCP_FakeA': 1, 'DFDCP_FakeB': 1, 'DFDC_Fake': 1, 'DFDC_Real': 0, 'DF_fake': 1, 'DF_real': 0, 'UADFV_Fake': 1, 'UADFV_Real': 0, 'roop_Real': 0, 'roop_Fake': 1, 'real': 0, 'fake': 1}
local_rank: 0
ddp: False

['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'groups', 'bias', 'padding_mode', 'device', 'dtype']
spatial_count=0 keep_stride_count=0
model_name: clip_enhanced
features_dim: 1024

ðŸ”¥ Trainable parameters:
feature_extractor.base_model.model.vision_model.pre_layrnorm.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.pre_layrnorm.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.12.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.12.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.12.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.12.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.13.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.13.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.13.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.13.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.14.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.14.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.14.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.14.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.15.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.15.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
2025-05-28 03:10:29,605 - INFO - AMP enabled with dtype=bf16, level=O1
2025-05-28 03:10:29,605 - INFO - ===> Epoch[0] start!
2025-05-28 03:10:29,631 - INFO - data_dict saved to ./logs/training/clip_enhanced_2025-05-28-03-10-27/train/FaceForensics++/data_dict_train.pickle
feature_extractor.base_model.model.vision_model.encoder.layers.15.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.15.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.16.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.16.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.16.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.16.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.17.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.17.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.17.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.17.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.18.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.18.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.18.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.18.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.19.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.19.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.19.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.19.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.20.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.20.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.20.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.20.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.21.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.21.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.21.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.21.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.22.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.22.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.22.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.22.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.23.layer_norm1.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.23.layer_norm1.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.23.layer_norm2.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.encoder.layers.23.layer_norm2.ln_tuning_layers.default.bias shape = (1024,)
feature_extractor.base_model.model.vision_model.post_layernorm.ln_tuning_layers.default.weight shape = (1024,)
feature_extractor.base_model.model.vision_model.post_layernorm.ln_tuning_layers.default.bias shape = (1024,)
model.linear.weight shape = (2, 1024)
model.linear.bias shape = (2,)
Total parameters: 303284226, trainable: 104450, %: 0.0344
  0%|          | 0/3591 [00:00<?, ?it/s]2025-05-28 03:10:33,666 - INFO - Iter: 0    training-loss, overall: 0.6761474609375    
2025-05-28 03:10:33,673 - INFO - Iter: 0    training-metric, acc: 0.75    training-metric, auc: 0.4427083333333333    training-metric, eer: 0.5    training-metric, ap: 0.7384684765702999    
  0%|          | 1/3591 [00:03<3:03:08,  3.06s/it]  0%|          | 2/3591 [00:04<2:22:40,  2.39s/it]  0%|          | 3/3591 [00:06<2:09:01,  2.16s/it]  0%|          | 4/3591 [00:08<2:02:34,  2.05s/it]  0%|          | 5/3591 [00:10<1:59:04,  1.99s/it]  0%|          | 6/3591 [00:12<1:57:05,  1.96s/it]  0%|          | 7/3591 [00:14<1:55:53,  1.94s/it]  0%|          | 8/3591 [00:16<1:55:11,  1.93s/it]  0%|          | 9/3591 [00:18<1:54:38,  1.92s/it]  0%|          | 10/3591 [00:20<1:54:26,  1.92s/it]  0%|          | 11/3591 [00:22<1:54:17,  1.92s/it]  0%|          | 12/3591 [00:23<1:54:06,  1.91s/it]  0%|          | 13/3591 [00:25<1:54:00,  1.91s/it]  0%|          | 14/3591 [00:27<1:53:57,  1.91s/it]  0%|          | 15/3591 [00:29<1:53:58,  1.91s/it]  0%|          | 16/3591 [00:31<1:53:57,  1.91s/it]  0%|          | 17/3591 [00:33<1:54:01,  1.91s/it]  1%|          | 18/3591 [00:35<1:54:05,  1.92s/it]  1%|          | 19/3591 [00:37<1:54:11,  1.92s/it]  1%|          | 20/3591 [00:39<1:54:10,  1.92s/it]  1%|          | 21/3591 [00:41<1:54:07,  1.92s/it]  1%|          | 22/3591 [00:43<1:54:21,  1.92s/it]  1%|          | 23/3591 [00:45<1:54:31,  1.93s/it]  1%|          | 24/3591 [00:47<1:54:28,  1.93s/it]  1%|          | 25/3591 [00:48<1:54:28,  1.93s/it]  1%|          | 26/3591 [00:50<1:54:26,  1.93s/it]  1%|          | 27/3591 [00:52<1:54:28,  1.93s/it]  1%|          | 28/3591 [00:54<1:54:27,  1.93s/it]  1%|          | 29/3591 [00:56<1:54:31,  1.93s/it]  1%|          | 30/3591 [00:58<1:54:28,  1.93s/it]  1%|          | 31/3591 [01:00<1:54:28,  1.93s/it]  1%|          | 32/3591 [01:02<1:54:26,  1.93s/it]  1%|          | 33/3591 [01:04<1:54:25,  1.93s/it]  1%|          | 34/3591 [01:06<1:54:25,  1.93s/it]  1%|          | 35/3591 [01:08<1:54:29,  1.93s/it]  1%|          | 36/3591 [01:10<1:54:27,  1.93s/it]  1%|          | 37/3591 [01:12<1:54:26,  1.93s/it]  1%|          | 38/3591 [01:14<1:54:24,  1.93s/it]  1%|          | 39/3591 [01:15<1:54:30,  1.93s/it]  1%|          | 40/3591 [01:17<1:55:13,  1.95s/it]  1%|          | 41/3591 [01:19<1:55:00,  1.94s/it]  1%|          | 42/3591 [01:21<1:54:47,  1.94s/it]  1%|          | 43/3591 [01:23<1:54:39,  1.94s/it]  1%|          | 44/3591 [01:25<1:54:34,  1.94s/it]  1%|â–         | 45/3591 [01:27<1:54:32,  1.94s/it]  1%|â–         | 46/3591 [01:29<1:54:38,  1.94s/it]  1%|â–         | 47/3591 [01:31<1:54:37,  1.94s/it]  1%|â–         | 48/3591 [01:33<1:54:34,  1.94s/it]  1%|â–         | 49/3591 [01:35<1:54:31,  1.94s/it]  1%|â–         | 50/3591 [01:37<1:54:33,  1.94s/it]  1%|â–         | 51/3591 [01:39<1:54:35,  1.94s/it]  1%|â–         | 52/3591 [01:41<1:54:36,  1.94s/it]  1%|â–         | 53/3591 [01:43<1:54:36,  1.94s/it]  2%|â–         | 54/3591 [01:45<1:54:34,  1.94s/it]  2%|â–         | 55/3591 [01:47<1:54:33,  1.94s/it]  2%|â–         | 56/3591 [01:48<1:54:30,  1.94s/it]  2%|â–         | 57/3591 [01:50<1:54:27,  1.94s/it]  2%|â–         | 58/3591 [01:52<1:54:23,  1.94s/it]  2%|â–         | 59/3591 [01:54<1:54:23,  1.94s/it]  2%|â–         | 60/3591 [01:56<1:54:25,  1.94s/it]  2%|â–         | 61/3591 [01:58<1:54:22,  1.94s/it]  2%|â–         | 62/3591 [02:00<1:54:23,  1.94s/it]  2%|â–         | 63/3591 [02:02<1:54:26,  1.95s/it]  2%|â–         | 64/3591 [02:04<1:54:28,  1.95s/it]  2%|â–         | 65/3591 [02:06<1:54:25,  1.95s/it]  2%|â–         | 66/3591 [02:08<1:54:21,  1.95s/it]  2%|â–         | 67/3591 [02:10<1:54:16,  1.95s/it]  2%|â–         | 68/3591 [02:12<1:54:18,  1.95s/it]  2%|â–         | 69/3591 [02:14<1:54:14,  1.95s/it]  2%|â–         | 70/3591 [02:16<1:54:13,  1.95s/it]  2%|â–         | 71/3591 [02:18<1:54:18,  1.95s/it]  2%|â–         | 72/3591 [02:20<1:54:14,  1.95s/it]  2%|â–         | 73/3591 [02:22<1:54:13,  1.95s/it]  2%|â–         | 74/3591 [02:24<1:54:08,  1.95s/it]  2%|â–         | 75/3591 [02:25<1:54:07,  1.95s/it]  2%|â–         | 76/3591 [02:27<1:54:03,  1.95s/it]  2%|â–         | 77/3591 [02:29<1:54:01,  1.95s/it]  2%|â–         | 78/3591 [02:31<1:54:00,  1.95s/it]  2%|â–         | 79/3591 [02:33<1:53:56,  1.95s/it]  2%|â–         | 80/3591 [02:35<1:53:51,  1.95s/it]  2%|â–         | 81/3591 [02:37<1:53:49,  1.95s/it]  2%|â–         | 82/3591 [02:39<1:53:51,  1.95s/it]  2%|â–         | 83/3591 [02:41<1:54:09,  1.95s/it]  2%|â–         | 84/3591 [02:43<1:54:04,  1.95s/it]  2%|â–         | 85/3591 [02:45<1:53:56,  1.95s/it]  2%|â–         | 86/3591 [02:47<1:53:51,  1.95s/it]  2%|â–         | 87/3591 [02:49<1:53:49,  1.95s/it]  2%|â–         | 88/3591 [02:51<1:53:45,  1.95s/it]