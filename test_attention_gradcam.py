#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•CLIP ViT Attentionå¯è§†åŒ–åŠŸèƒ½
è¿™ä¸ªè„šæœ¬ä¼šåˆ›å»ºåˆæˆæ•°æ®æ¥æµ‹è¯•GradCAMæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
sys.path.append('/root/autodl-tmp/benchmark_deepfakes/DeepfakeBench')

import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import yaml

from training.utils.clip_vit_gradcam import CLIPViTGradCAM
from training.detectors.clip_enhanced import CLIPEnhanced

# ç¡®ä¿å¯ä»¥æ‰¾åˆ°trainingæ¨¡å—
sys.path.append('/root/autodl-tmp/benchmark_deepfakes/DeepfakeBench/training')

def create_test_config():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„é…ç½®"""
    config = {
        'backbone': 'ViT-L/14',
        'clip_path': 'weights/clip-vit-base-patch16',
        'mlp_layer': 1,
        'loss_func': 'cross_entropy'  # ä¿®æ­£losså‡½æ•°åç§°
    }
    return config

def create_synthetic_image(size=(224, 224)):
    """åˆ›å»ºåˆæˆæµ‹è¯•å›¾åƒ"""
    # åˆ›å»ºä¸€ä¸ªæœ‰ç»“æ„çš„æµ‹è¯•å›¾åƒ
    image = np.zeros((*size, 3), dtype=np.uint8)
    
    # æ·»åŠ ä¸€äº›å‡ ä½•å½¢çŠ¶æ¥æµ‹è¯•attention
    center_x, center_y = size[0] // 2, size[1] // 2
    
    # çº¢è‰²åœ†å½¢
    y, x = np.ogrid[:size[0], :size[1]]
    mask = (x - center_x)**2 + (y - center_y)**2 <= 40**2
    image[mask] = [255, 100, 100]
    
    # è“è‰²æ–¹å½¢
    image[50:100, 50:100] = [100, 100, 255]
    
    # ç»¿è‰²ä¸‰è§’å½¢åŒºåŸŸ
    for i in range(150, 200):
        for j in range(150, 150 + (i - 150)):
            if j < size[1]:
                image[i, j] = [100, 255, 100]
    
    # æ·»åŠ ä¸€äº›å™ªå£°
    noise = np.random.randint(0, 50, size=(*size, 3))
    image = np.clip(image.astype(int) + noise, 0, 255).astype(np.uint8)
    
    return image

def test_gradcam_basic():
    """åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹åŸºç¡€åŠŸèƒ½æµ‹è¯•...")
    
    try:
        # åˆ›å»ºæµ‹è¯•é…ç½®å’Œæ¨¡å‹
        config = create_test_config()
        print("âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥CLIPè·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(config['clip_path']):
            print(f"âš ï¸  CLIPæƒé‡ä¸å­˜åœ¨: {config['clip_path']}")
            print("   ä½¿ç”¨éšæœºåˆå§‹åŒ–è¿›è¡Œæµ‹è¯•...")
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆå¯èƒ½ä¼šå¤±è´¥å¦‚æœæ²¡æœ‰æƒé‡ï¼‰
        try:
            model = CLIPEnhanced(config)
            model.eval()
            print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return False
        
        # åˆ›å»ºGradCAM
        try:
            gradcam = CLIPViTGradCAM(model, target_layers=[-2, -1], head_fusion="mean")
            print("âœ… GradCAMåˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ GradCAMåˆ›å»ºå¤±è´¥: {e}")
            return False
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_image = create_synthetic_image()
        test_tensor = torch.from_numpy(test_image).permute(2, 0, 1).float() / 255.0
        test_tensor = test_tensor.unsqueeze(0)  # [1, 3, 224, 224]
        print("âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        try:
            with torch.no_grad():
                data_dict = {'image': test_tensor}
                pred_dict = model(data_dict, inference=True)
                print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {pred_dict['cls'].shape}")
                print(f"   é¢„æµ‹æ¦‚ç‡: {pred_dict['prob'].item():.3f}")
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•GradCAMç”Ÿæˆ
        try:
            cam = gradcam.generate_cam(test_tensor, target_class=1)
            print(f"âœ… CAMç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {cam.shape}")
            print(f"   CAMå€¼èŒƒå›´: [{cam.min():.3f}, {cam.max():.3f}]")
        except Exception as e:
            print(f"âŒ CAMç”Ÿæˆå¤±è´¥: {e}")
            gradcam.cleanup()
            return False
        
        # æµ‹è¯•å¯è§†åŒ–
        try:
            visualization = gradcam.visualize_cam(test_image, cam[0])
            print(f"âœ… å¯è§†åŒ–æˆåŠŸï¼Œå½¢çŠ¶: {visualization.shape}")
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")
            gradcam.cleanup()
            return False
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        try:
            os.makedirs('test_results', exist_ok=True)
            
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))
            
            axes[0].imshow(test_image)
            axes[0].set_title('Synthetic Test Image')
            axes[0].axis('off')
            
            axes[1].imshow(cam[0], cmap='jet')
            axes[1].set_title('Attention Heatmap')
            axes[1].axis('off')
            
            axes[2].imshow(visualization)
            axes[2].set_title('Overlay Visualization')
            axes[2].axis('off')
            
            plt.suptitle('CLIP ViT GradCAM Test Results', fontsize=16)
            plt.tight_layout()
            plt.savefig('test_results/gradcam_test.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print("âœ… æµ‹è¯•ç»“æœä¿å­˜æˆåŠŸ: test_results/gradcam_test.png")
        except Exception as e:
            print(f"âŒ ç»“æœä¿å­˜å¤±è´¥: {e}")
        
        # æ¸…ç†
        gradcam.cleanup()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºç°æœªé¢„æœŸé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_multiple_layers():
    """å¤šå±‚attentionæµ‹è¯•"""
    print("\nğŸ”¬ å¼€å§‹å¤šå±‚attentionæµ‹è¯•...")
    
    try:
        config = create_test_config()
        
        if not os.path.exists(config['clip_path']):
            print("âš ï¸  è·³è¿‡å¤šå±‚æµ‹è¯•ï¼ˆæ²¡æœ‰CLIPæƒé‡ï¼‰")
            return True
            
        model = CLIPEnhanced(config)
        model.eval()
        
        # æµ‹è¯•ä¸åŒçš„å±‚é…ç½®
        layer_configs = [
            ([-1], "last_layer"),
            ([-2, -1], "last_2_layers"),
            ([-4, -3, -2, -1], "last_4_layers")
        ]
        
        test_image = create_synthetic_image()
        test_tensor = torch.from_numpy(test_image).permute(2, 0, 1).float() / 255.0
        test_tensor = test_tensor.unsqueeze(0)
        
        results = {}
        
        for target_layers, name in layer_configs:
            print(f"   æµ‹è¯•é…ç½®: {name}")
            
            gradcam = CLIPViTGradCAM(model, target_layers=target_layers)
            try:
                cam = gradcam.generate_cam(test_tensor, target_class=1)
                results[name] = cam[0]
                print(f"   âœ… {name} æˆåŠŸ")
            except Exception as e:
                print(f"   âŒ {name} å¤±è´¥: {e}")
            finally:
                gradcam.cleanup()
        
        if results:
            # ä¿å­˜å¤šå±‚å¯¹æ¯”ç»“æœ
            fig, axes = plt.subplots(1, len(results), figsize=(5*len(results), 5))
            if len(results) == 1:
                axes = [axes]
            
            for i, (name, cam) in enumerate(results.items()):
                axes[i].imshow(cam, cmap='jet')
                axes[i].set_title(f'{name}\nRange: [{cam.min():.2f}, {cam.max():.2f}]')
                axes[i].axis('off')
            
            plt.suptitle('Multi-Layer Attention Comparison', fontsize=16)
            plt.tight_layout()
            plt.savefig('test_results/multilayer_test.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print("âœ… å¤šå±‚æµ‹è¯•ç»“æœä¿å­˜: test_results/multilayer_test.png")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šå±‚æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_head_fusion():
    """æµ‹è¯•ä¸åŒçš„å¤šå¤´èåˆæ–¹æ³•"""
    print("\nğŸ¯ å¼€å§‹å¤šå¤´èåˆæµ‹è¯•...")
    
    try:
        config = create_test_config()
        
        if not os.path.exists(config['clip_path']):
            print("âš ï¸  è·³è¿‡å¤šå¤´èåˆæµ‹è¯•ï¼ˆæ²¡æœ‰CLIPæƒé‡ï¼‰")
            return True
            
        model = CLIPEnhanced(config)
        model.eval()
        
        test_image = create_synthetic_image()
        test_tensor = torch.from_numpy(test_image).permute(2, 0, 1).float() / 255.0
        test_tensor = test_tensor.unsqueeze(0)
        
        fusion_methods = ["mean", "max", "min"]
        results = {}
        
        for method in fusion_methods:
            print(f"   æµ‹è¯•èåˆæ–¹æ³•: {method}")
            
            gradcam = CLIPViTGradCAM(model, target_layers=[-2, -1], head_fusion=method)
            try:
                cam = gradcam.generate_cam(test_tensor, target_class=1)
                results[method] = cam[0]
                print(f"   âœ… {method} æˆåŠŸ")
            except Exception as e:
                print(f"   âŒ {method} å¤±è´¥: {e}")
            finally:
                gradcam.cleanup()
        
        if results:
            # ä¿å­˜èåˆæ–¹æ³•å¯¹æ¯”
            fig, axes = plt.subplots(1, len(results), figsize=(5*len(results), 5))
            if len(results) == 1:
                axes = [axes]
            
            for i, (method, cam) in enumerate(results.items()):
                axes[i].imshow(cam, cmap='jet')
                axes[i].set_title(f'{method.upper()} Fusion\nRange: [{cam.min():.2f}, {cam.max():.2f}]')
                axes[i].axis('off')
            
            plt.suptitle('Head Fusion Methods Comparison', fontsize=16)
            plt.tight_layout()
            plt.savefig('test_results/head_fusion_test.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print("âœ… å¤šå¤´èåˆæµ‹è¯•ç»“æœä¿å­˜: test_results/head_fusion_test.png")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šå¤´èåˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹CLIP ViT GradCAMå®Œæ•´æµ‹è¯•...")
    print("="*60)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("åŸºç¡€åŠŸèƒ½æµ‹è¯•", test_gradcam_basic),
        ("å¤šå±‚attentionæµ‹è¯•", test_multiple_layers),
        ("å¤šå¤´èåˆæµ‹è¯•", test_head_fusion)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{test_name}...")
        result = test_func()
        results.append((test_name, result))
        print("-" * 40)
    
    # æ€»ç»“ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    
    all_passed = True
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GradCAMåŠŸèƒ½æ­£å¸¸ã€‚")
        print("ğŸ“ æŸ¥çœ‹æµ‹è¯•ç»“æœ: test_results/ ç›®å½•")
        print("ğŸ“– ä½¿ç”¨æŒ‡å—: examples/README_attention_visualization.md")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        print("ğŸ’¡ å¸¸è§é—®é¢˜:")
        print("   - ç¡®ä¿CLIPæ¨¡å‹æƒé‡å­˜åœ¨")
        print("   - æ£€æŸ¥CUDAå†…å­˜æ˜¯å¦è¶³å¤Ÿ")
        print("   - ç¡®è®¤æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")

if __name__ == "__main__":
    main()
