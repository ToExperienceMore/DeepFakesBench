import torch
import torch.nn.functional as F
import numpy as np
import cv2
import matplotlib.pyplot as plt
from typing import List, Optional, Tuple, Dict
import warnings

class CLIPViTGradCAM:
    """
    ViT Gradient-weighted Attention CAM for CLIP models
    
    å®ç°åŸç†ï¼š
    1. æ³¨å†Œattentionå±‚çš„forwardå’Œbackward hooks
    2. å‰å‘ä¼ æ’­æ—¶ä¿å­˜attention weights
    3. åå‘ä¼ æ’­æ—¶è®¡ç®—attentionçš„æ¢¯åº¦
    4. ç”¨æ¢¯åº¦åŠ æƒattention weights
    5. å°†patch-levelé‡è¦æ€§æ˜ å°„å›åŸå›¾
    """
    
    def __init__(self, model, target_layers: Optional[List[int]] = None, head_fusion: str = "mean"):
        """
        Args:
            model: CLIPEnhanced model
            target_layers: è¦åˆ†æçš„transformerå±‚ç´¢å¼•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æœ€åå‡ å±‚
            head_fusion: å¤šå¤´attentionèåˆæ–¹æ³• ("mean", "max", "min")
        """
        self.model = model
        # å…ˆåªç”¨æœ€åä¸€å±‚è¿›è¡Œè°ƒè¯•
        self.target_layers = target_layers or [-1]  # å…ˆåªæµ‹è¯•æœ€åä¸€å±‚
        self.head_fusion = head_fusion
        
        print(f"ğŸ¯ åˆå§‹åŒ–GradCAMï¼Œç›®æ ‡å±‚: {self.target_layers}")
        
        # å­˜å‚¨æ³¨å†Œçš„hooks
        self.hooks = []
        
        # å­˜å‚¨attention weightså’Œgradients
        self.attention_weights = {}
        self.attention_gradients = {}
        
        # è·å–CLIP vision model
        self.clip_vision = self._get_clip_vision_model()
        
        # æ³¨å†Œhooks
        self._register_hooks()
        
    def _get_clip_vision_model(self):
        """è·å–CLIP vision transformeræ¨¡å‹"""
        # å¤„ç†PEFTåŒ…è£…çš„æ¨¡å‹
        if hasattr(self.model.feature_extractor, 'base_model'):
            clip_vision = self.model.feature_extractor.base_model.vision_model
        else:
            clip_vision = self.model.feature_extractor.vision_model
            
        print(f"ğŸ” CLIP Vision Model Info:")
        print(f"   - Type: {type(clip_vision)}")
        print(f"   - Has encoder: {hasattr(clip_vision, 'encoder')}")
        if hasattr(clip_vision, 'encoder'):
            print(f"   - Encoder layers: {len(clip_vision.encoder.layers)}")
            
        return clip_vision
    
    def _register_hooks(self):
        """æ³¨å†Œattentionå±‚çš„hooks"""
        encoder_layers = self.clip_vision.encoder.layers
        
        for i, layer_idx in enumerate(self.target_layers):
            # å¤„ç†è´Ÿç´¢å¼•
            if layer_idx < 0:
                layer_idx = len(encoder_layers) + layer_idx
                
            layer = encoder_layers[layer_idx]
            attention_module = layer.self_attn
            
            # æ³¨å†Œforward hookè®¡ç®—å¹¶ä¿å­˜attention weights
            def save_attention_hook(layer_name):
                def hook(module, input, output):
                    try:
                        print(f"ğŸ” Hook triggered for {layer_name}")
                        print(f"   - Input type: {type(input)}")
                        print(f"   - Input length: {len(input) if isinstance(input, (tuple, list)) else 'not sequence'}")
                        print(f"   - Output type: {type(output)}")
                        print(f"   - Output length: {len(output) if isinstance(output, (tuple, list)) else 'not sequence'}")
                        
                        # å°è¯•å¤šç§æ–¹å¼è·å–hidden_states
                        hidden_states = None
                        
                        if isinstance(input, tuple) and len(input) > 0:
                            hidden_states = input[0]
                        elif isinstance(input, torch.Tensor):
                            hidden_states = input
                        else:
                            print(f"âŒ Cannot extract hidden_states from input for {layer_name}")
                            return
                        
                        if not isinstance(hidden_states, torch.Tensor):
                            print(f"âŒ hidden_states is not a tensor for {layer_name}: {type(hidden_states)}")
                            return
                            
                        print(f"   - Hidden states shape: {hidden_states.shape}")
                        
                        # æ£€æŸ¥ç»´åº¦
                        if len(hidden_states.shape) != 3:
                            print(f"âŒ Unexpected hidden_states shape for {layer_name}: {hidden_states.shape}")
                            return
                            
                        B, L, D = hidden_states.shape
                        print(f"   - Batch size: {B}, Sequence length: {L}, Hidden dim: {D}")
                        
                        # æ£€æŸ¥æ¨¡å—å±æ€§
                        if not hasattr(module, 'q_proj') or not hasattr(module, 'k_proj'):
                            print(f"âŒ Module missing q_proj or k_proj for {layer_name}")
                            return
                        
                        # è·å–Q, K, V
                        query = module.q_proj(hidden_states)  # [B, L, D]
                        key = module.k_proj(hidden_states)    # [B, L, D]
                        
                        print(f"   - Query shape: {query.shape}, Key shape: {key.shape}")
                        
                        # æ£€æŸ¥æ¨¡å—é…ç½®
                        num_heads = getattr(module, 'num_heads', 12)
                        head_dim = getattr(module, 'head_dim', D // num_heads)
                        
                        print(f"   - Num heads: {num_heads}, Head dim: {head_dim}")
                        
                        # Reshape to multi-head format
                        query = query.view(B, L, num_heads, head_dim).transpose(1, 2)  # [B, H, L, head_dim]
                        key = key.view(B, L, num_heads, head_dim).transpose(1, 2)      # [B, H, L, head_dim]
                        
                        # è®¡ç®—attention scores
                        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)  # [B, H, L, L]
                        attention_probs = torch.softmax(attention_scores, dim=-1)  # [B, H, L, L]
                        
                        # ç¡®ä¿tensoréœ€è¦æ¢¯åº¦
                        if attention_probs.requires_grad:
                            self.attention_weights[layer_name] = attention_probs
                        else:
                            self.attention_weights[layer_name] = attention_probs.detach().requires_grad_(True)
                            
                        print(f"âœ… Successfully computed attention for {layer_name}: {attention_probs.shape}")
                            
                    except Exception as e:
                        print(f"âŒ Failed to compute attention for {layer_name}: {e}")
                        import traceback
                        traceback.print_exc()
                        
                return hook
            
            # æ³¨å†Œbackward hookä¿å­˜gradients
            def save_gradient_hook(layer_name):
                def hook(module, grad_input, grad_output):
                    # ä¿å­˜attention weightsçš„æ¢¯åº¦
                    if layer_name in self.attention_weights and self.attention_weights[layer_name] is not None:
                        if self.attention_weights[layer_name].requires_grad:
                            def grad_hook(grad):
                                self.attention_gradients[layer_name] = grad
                                return grad
                            
                            self.attention_weights[layer_name].register_hook(grad_hook)
                return hook
            
            layer_name = f"layer_{layer_idx}"
            
            # æ³¨å†Œhooks
            forward_hook = attention_module.register_forward_hook(save_attention_hook(layer_name))
            backward_hook = attention_module.register_full_backward_hook(save_gradient_hook(layer_name))
            
            self.hooks.extend([forward_hook, backward_hook])
    
    def _fuse_attention_heads(self, attention: torch.Tensor) -> torch.Tensor:
        """
        èåˆå¤šå¤´attention
        Args:
            attention: [B, H, L, L] å¤šå¤´attentionæƒé‡
        Returns:
            fused_attention: [B, L, L] èåˆåçš„attention
        """
        if self.head_fusion == "mean":
            return attention.mean(dim=1)
        elif self.head_fusion == "max":
            return attention.max(dim=1)[0]
        elif self.head_fusion == "min":
            return attention.min(dim=1)[0]
        else:
            raise ValueError(f"Unknown head fusion method: {self.head_fusion}")
    
    def _compute_gradient_weighted_attention(self) -> torch.Tensor:
        """
        è®¡ç®—æ¢¯åº¦åŠ æƒçš„attention
        Returns:
            cam: [B, H, W] çƒ­åŠ›å›¾
        """
        if not self.attention_weights or not self.attention_gradients:
            raise ValueError("No attention weights or gradients found. Make sure to run forward and backward first.")
        
        batch_size = None
        weighted_attentions = []
        
        for layer_name in self.attention_weights.keys():
            if layer_name not in self.attention_gradients:
                warnings.warn(f"No gradients found for layer {layer_name}, skipping...")
                continue
                
            attention = self.attention_weights[layer_name]  # [B, H, L, L]
            gradients = self.attention_gradients[layer_name]  # [B, H, L, L]
            
            if batch_size is None:
                batch_size = attention.size(0)
            
            # æ¢¯åº¦åŠ æƒattention
            weighted_attention = attention * gradients  # [B, H, L, L]
            
            # èåˆå¤šå¤´
            fused_attention = self._fuse_attention_heads(weighted_attention)  # [B, L, L]
            
            # å–CLS tokenå¯¹patchçš„attention (å»æ‰CLS token)
            cls_attention = fused_attention[:, 0, 1:]  # [B, L-1] = [B, 196]
            
            weighted_attentions.append(cls_attention)
        
        if not weighted_attentions:
            raise ValueError("No valid weighted attentions computed.")
        
        # å¹³å‡å¤šå±‚attention
        final_attention = torch.stack(weighted_attentions).mean(dim=0)  # [B, 196]
        
        # é‡å¡‘ä¸ºspatialç»´åº¦ (å‡è®¾æ˜¯14x14çš„patch grid)
        patch_size = int(np.sqrt(final_attention.size(-1)))  # 14
        spatial_attention = final_attention.view(batch_size, patch_size, patch_size)  # [B, 14, 14]
        
        return spatial_attention
    
    def generate_cam(self, input_tensor: torch.Tensor, target_class: Optional[int] = None, 
                     retain_graph: bool = False) -> np.ndarray:
        """
        ç”ŸæˆCAMçƒ­åŠ›å›¾
        Args:
            input_tensor: è¾“å…¥å›¾åƒtensor [B, C, H, W]
            target_class: ç›®æ ‡ç±»åˆ«ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é¢„æµ‹çš„ç±»åˆ«
            retain_graph: æ˜¯å¦ä¿ç•™è®¡ç®—å›¾
        Returns:
            cam: [B, H, W] numpyçƒ­åŠ›å›¾ï¼Œå€¼åœ¨[0,1]èŒƒå›´
        """
        # æ¸…ç©ºä¹‹å‰çš„è®°å½•
        self.attention_weights.clear()
        self.attention_gradients.clear()
        
        # è®¾ç½®æ¨¡å‹ä¸ºevalæ¨¡å¼ä½†ä¿æŒrequires_grad
        self.model.eval()
        input_tensor.requires_grad_(True)
        
        # å‰å‘ä¼ æ’­
        data_dict = {'image': input_tensor}
        pred_dict = self.model(data_dict, inference=True)
        
        # è·å–é¢„æµ‹å’Œç›®æ ‡ç±»åˆ«
        predictions = pred_dict['cls']  # [B, num_classes]
        
        if target_class is None:
            target_class = predictions.argmax(dim=1)
        elif isinstance(target_class, int):
            target_class = torch.tensor([target_class] * predictions.size(0), device=predictions.device)
        
        # æ¸…é›¶æ¢¯åº¦
        self.model.zero_grad()
        
        # åå‘ä¼ æ’­
        target_scores = predictions.gather(1, target_class.unsqueeze(1)).squeeze(1)
        loss = target_scores.sum()
        loss.backward(retain_graph=retain_graph)
        
        # è®¡ç®—æ¢¯åº¦åŠ æƒattention
        try:
            spatial_attention = self._compute_gradient_weighted_attention()  # [B, 14, 14]
        except ValueError as e:
            print(f"âŒ Error computing attention: {e}")
            print(f"ğŸ” Debug info:")
            print(f"   - Attention weights collected: {list(self.attention_weights.keys())}")
            print(f"   - Attention gradients collected: {list(self.attention_gradients.keys())}")
            print(f"   - Target layers: {self.target_layers}")
            
            # è¿”å›å…¨é›¶çƒ­åŠ›å›¾
            batch_size, _, height, width = input_tensor.shape
            print(f"âš ï¸  Returning zero CAM due to attention computation failure")
            return np.zeros((batch_size, height, width))
        
        # åº”ç”¨ReLUå’Œå½’ä¸€åŒ–
        spatial_attention = F.relu(spatial_attention)
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        batch_size = spatial_attention.size(0)
        for i in range(batch_size):
            attention_i = spatial_attention[i]
            min_val = attention_i.min()
            max_val = attention_i.max()
            if max_val > min_val:
                spatial_attention[i] = (attention_i - min_val) / (max_val - min_val)
        
        # ä¸Šé‡‡æ ·åˆ°åŸå›¾å°ºå¯¸
        input_height, input_width = input_tensor.shape[2], input_tensor.shape[3]
        cam = F.interpolate(
            spatial_attention.unsqueeze(1),  # [B, 1, 14, 14]
            size=(input_height, input_width),
            mode='bilinear',
            align_corners=False
        ).squeeze(1)  # [B, H, W]
        
        return cam.detach().cpu().numpy()
    
    def visualize_cam(self, image: np.ndarray, cam: np.ndarray, alpha: float = 0.4,
                     colormap: int = cv2.COLORMAP_JET) -> np.ndarray:
        """
        å¯è§†åŒ–CAMçƒ­åŠ›å›¾
        Args:
            image: åŸå§‹å›¾åƒ [H, W, C] æˆ– [H, W]ï¼Œå€¼åœ¨[0,255]
            cam: CAMçƒ­åŠ›å›¾ [H, W]ï¼Œå€¼åœ¨[0,1]
            alpha: çƒ­åŠ›å›¾é€æ˜åº¦
            colormap: OpenCVé¢œè‰²æ˜ å°„
        Returns:
            visualization: å¯è§†åŒ–ç»“æœ [H, W, C]
        """
        # ç¡®ä¿imageæ˜¯3é€šé“
        if len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        elif len(image.shape) == 3 and image.shape[2] == 3:
            image = image.copy()
        
        # å½’ä¸€åŒ–imageåˆ°[0,1]
        if image.max() > 1:
            image = image.astype(np.float32) / 255.0
        
        # ç¡®ä¿CAMå’Œimageå°ºå¯¸ä¸€è‡´
        if cam.shape != image.shape[:2]:
            # å°†CAM resizeåˆ°imageçš„å°ºå¯¸
            cam = cv2.resize(cam, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)
            print(f"ğŸ“ Resized CAM from shape to {cam.shape} to match image shape {image.shape[:2]}")
        
        # ç”Ÿæˆçƒ­åŠ›å›¾
        heatmap = cv2.applyColorMap(np.uint8(255 * cam), colormap)
        heatmap = np.float32(heatmap) / 255.0
        
        # ç¡®ä¿RGBé¡ºåº
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        # å åŠ çƒ­åŠ›å›¾
        visualization = alpha * heatmap + (1 - alpha) * image
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        visualization = np.clip(visualization, 0, 1)
        
        return visualization
    
    def generate_and_visualize(self, input_tensor: torch.Tensor, original_image: np.ndarray,
                              target_class: Optional[int] = None, save_path: Optional[str] = None) -> Dict:
        """
        ä¸€é”®ç”Ÿæˆå¹¶å¯è§†åŒ–CAM
        Args:
            input_tensor: è¾“å…¥tensor [1, C, H, W] 
            original_image: åŸå§‹å›¾åƒç”¨äºå¯è§†åŒ– [H, W, C]
            target_class: ç›®æ ‡ç±»åˆ«
            save_path: ä¿å­˜è·¯å¾„
        Returns:
            results: åŒ…å«é¢„æµ‹ã€CAMå’Œå¯è§†åŒ–çš„å­—å…¸
        """
        # ç”ŸæˆCAM
        cam = self.generate_cam(input_tensor, target_class)[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        
        # è·å–é¢„æµ‹
        self.model.eval()
        with torch.no_grad():
            data_dict = {'image': input_tensor}
            pred_dict = self.model(data_dict, inference=True)
            predictions = pred_dict['prob'].cpu().numpy()[0]  # Fakeæ¦‚ç‡
            predicted_class = int(predictions > 0.5)
        
        # å¯è§†åŒ–
        visualization = self.visualize_cam(original_image, cam)
        
        # åˆ›å»ºç»“æœå›¾
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # åŸå›¾
        axes[0].imshow(original_image)
        axes[0].set_title('Original Image')
        axes[0].axis('off')
        
        # çƒ­åŠ›å›¾
        axes[1].imshow(cam, cmap='jet')
        axes[1].set_title(f'Attention Heatmap\nFake Prob: {predictions:.3f}')
        axes[1].axis('off')
        
        # å åŠ å›¾
        axes[2].imshow(visualization)
        axes[2].set_title(f'Overlay\nPredicted: {"Fake" if predicted_class else "Real"}')
        axes[2].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Visualization saved to: {save_path}")
        
        plt.show()
        
        return {
            'cam': cam,
            'visualization': visualization,
            'fake_probability': predictions,
            'predicted_class': predicted_class,
            'target_class': target_class
        }
    
    def cleanup(self):
        """æ¸…ç†æ³¨å†Œçš„hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        self.attention_weights.clear()
        self.attention_gradients.clear()
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œè‡ªåŠ¨æ¸…ç†"""
        self.cleanup()

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_clip_vit_gradcam():
    """æµ‹è¯•å‡½æ•°"""
    print("Testing CLIP ViT GradCAM...")
    
    # è¿™é‡Œåº”è¯¥åŠ è½½ä½ çš„å®é™…æ¨¡å‹
    # model = load_your_clip_enhanced_model()
    # gradcam = CLIPViTGradCAM(model)
    
    # # å‡†å¤‡æµ‹è¯•å›¾åƒ
    # image_tensor = torch.randn(1, 3, 224, 224)
    # original_image = np.random.randint(0, 255, (224, 224, 3)).astype(np.uint8)
    
    # # ç”Ÿæˆå¯è§†åŒ–
    # results = gradcam.generate_and_visualize(
    #     image_tensor, 
    #     original_image,
    #     save_path='test_gradcam.png'
    # )
    
    print("GradCAM implementation ready!")

if __name__ == "__main__":
    test_clip_vit_gradcam()
