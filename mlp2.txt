+ python3 training/test.py --detector_path ./training/config/detector/clip_enhanced.yaml --test_dataset DFDC FaceForensics++ DFDCP Celeb-DF-v2 UADFV FaceShifter DeepFakeDetection --weights_path ./logs/training/clip_enhanced_2025-06-01-19-22-52/test/avg/ckpt_best.pth
/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'groups', 'bias', 'padding_mode', 'device', 'dtype']
spatial_count=0 keep_stride_count=0
batch_size: 8
clip_path: weights/clip-vit-base-patch16
features_dim: 768
mlp_layer: 2

ðŸ”¥ Trainable parameters:
feature_extractor.base_model.model.vision_model.pre_layrnorm.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.pre_layrnorm.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.0.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.1.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.2.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.3.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.4.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.5.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.6.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.7.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.8.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.9.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.10.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm1.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm1.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm2.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.encoder.layers.11.layer_norm2.ln_tuning_layers.default.bias shape = (768,)
feature_extractor.base_model.model.vision_model.post_layernorm.ln_tuning_layers.default.weight shape = (768,)
feature_extractor.base_model.model.vision_model.post_layernorm.ln_tuning_layers.default.bias shape = (768,)
model.linear.weight shape = (2, 768)
model.linear.bias shape = (2,)
Total parameters: 85840898, trainable: 41474, %: 0.0483
Traceback (most recent call last):
  File "/root/autodl-tmp/benchmark_deepfakes/DeepfakeBench/training/test.py", line 356, in <module>
    main()
  File "/root/autodl-tmp/benchmark_deepfakes/DeepfakeBench/training/test.py", line 344, in main
    model.load_state_dict(state_dict, strict=True)
  File "/root/autodl-tmp/envs/DeepfakeBench-torch2.0/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for CLIPEnhanced:
	Missing key(s) in state_dict: "model.linear.weight", "model.linear.bias". 
	Unexpected key(s) in state_dict: "model.mlp.0.weight", "model.mlp.0.bias", "model.mlp.2.weight", "model.mlp.2.bias". 
